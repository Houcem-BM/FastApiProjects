2024-11-10 16:50:57,076:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 16:51:01,425:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 16:51:01,425:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 16:51:01,425:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 16:51:01,426:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 16:51:03,550:INFO:PyCaret ClassificationExperiment
2024-11-10 16:51:03,550:INFO:Logging name: clf-default-name
2024-11-10 16:51:03,550:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-10 16:51:03,550:INFO:version 3.3.2
2024-11-10 16:51:03,550:INFO:Initializing setup()
2024-11-10 16:51:03,550:INFO:self.USI: a2b5
2024-11-10 16:51:03,550:INFO:self._variable_keys: {'log_plots_param', '_ml_usecase', 'y_train', 'data', 'USI', 'is_multiclass', 'fix_imbalance', 'seed', 'memory', 'html_param', 'exp_name_log', 'fold_groups_param', 'idx', 'y_test', 'y', 'fold_generator', 'pipeline', 'gpu_n_jobs_param', 'n_jobs_param', 'gpu_param', 'logging_param', 'X_train', 'X', 'fold_shuffle_param', '_available_plots', 'exp_id', 'target_param', 'X_test'}
2024-11-10 16:51:03,550:INFO:Checking environment
2024-11-10 16:51:03,550:INFO:python_version: 3.10.12
2024-11-10 16:51:03,550:INFO:python_build: ('main', 'Sep 11 2024 15:47:36')
2024-11-10 16:51:03,550:INFO:machine: x86_64
2024-11-10 16:51:03,551:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 16:51:03,552:INFO:Memory: svmem(total=16587202560, available=9266573312, percent=44.1, used=6423093248, free=335667200, active=11322511360, inactive=3573833728, buffers=273498112, cached=9554944000, shared=549507072, slab=896036864)
2024-11-10 16:51:03,552:INFO:Physical Core: 6
2024-11-10 16:51:03,552:INFO:Logical Core: 12
2024-11-10 16:51:03,552:INFO:Checking libraries
2024-11-10 16:51:03,552:INFO:System:
2024-11-10 16:51:03,552:INFO:    python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
2024-11-10 16:51:03,552:INFO:executable: /usr/bin/python3
2024-11-10 16:51:03,552:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 16:51:03,552:INFO:PyCaret required dependencies:
2024-11-10 16:51:03,569:INFO:                 pip: 22.0.2
2024-11-10 16:51:03,569:INFO:          setuptools: 59.6.0
2024-11-10 16:51:03,569:INFO:             pycaret: 3.3.2
2024-11-10 16:51:03,569:INFO:             IPython: 7.31.1
2024-11-10 16:51:03,569:INFO:          ipywidgets: 8.1.5
2024-11-10 16:51:03,569:INFO:                tqdm: 4.67.0
2024-11-10 16:51:03,569:INFO:               numpy: 1.21.5
2024-11-10 16:51:03,569:INFO:              pandas: 2.1.4
2024-11-10 16:51:03,569:INFO:              jinja2: 3.0.3
2024-11-10 16:51:03,569:INFO:               scipy: 1.8.0
2024-11-10 16:51:03,569:INFO:              joblib: 1.3.2
2024-11-10 16:51:03,570:INFO:             sklearn: 1.4.2
2024-11-10 16:51:03,570:INFO:                pyod: 2.0.2
2024-11-10 16:51:03,570:INFO:            imblearn: 0.12.4
2024-11-10 16:51:03,570:INFO:   category_encoders: 2.6.4
2024-11-10 16:51:03,570:INFO:            lightgbm: 4.5.0
2024-11-10 16:51:03,570:INFO:               numba: 0.60.0
2024-11-10 16:51:03,570:INFO:            requests: 2.25.1
2024-11-10 16:51:03,570:INFO:          matplotlib: 3.5.1
2024-11-10 16:51:03,570:INFO:          scikitplot: 0.3.7
2024-11-10 16:51:03,570:INFO:         yellowbrick: 1.5
2024-11-10 16:51:03,570:INFO:              plotly: 5.24.1
2024-11-10 16:51:03,570:INFO:    plotly-resampler: Not installed
2024-11-10 16:51:03,570:INFO:             kaleido: 0.2.1
2024-11-10 16:51:03,570:INFO:           schemdraw: 0.15
2024-11-10 16:51:03,570:INFO:         statsmodels: 0.14.4
2024-11-10 16:51:03,570:INFO:              sktime: 0.26.0
2024-11-10 16:51:03,570:INFO:               tbats: 1.1.3
2024-11-10 16:51:03,570:INFO:            pmdarima: 2.0.4
2024-11-10 16:51:03,570:INFO:              psutil: 6.1.0
2024-11-10 16:51:03,570:INFO:          markupsafe: 2.0.1
2024-11-10 16:51:03,570:INFO:             pickle5: Not installed
2024-11-10 16:51:03,570:INFO:         cloudpickle: 3.0.0
2024-11-10 16:51:03,570:INFO:         deprecation: 2.1.0
2024-11-10 16:51:03,570:INFO:              xxhash: 3.5.0
2024-11-10 16:51:03,570:INFO:           wurlitzer: 3.1.1
2024-11-10 16:51:03,570:INFO:PyCaret optional dependencies:
2024-11-10 16:51:03,914:INFO:                shap: Not installed
2024-11-10 16:51:03,914:INFO:           interpret: Not installed
2024-11-10 16:51:03,914:INFO:                umap: Not installed
2024-11-10 16:51:03,914:INFO:     ydata_profiling: Not installed
2024-11-10 16:51:03,914:INFO:  explainerdashboard: Not installed
2024-11-10 16:51:03,914:INFO:             autoviz: Not installed
2024-11-10 16:51:03,915:INFO:           fairlearn: Not installed
2024-11-10 16:51:03,915:INFO:          deepchecks: Not installed
2024-11-10 16:51:03,915:INFO:             xgboost: Not installed
2024-11-10 16:51:03,915:INFO:            catboost: Not installed
2024-11-10 16:51:03,915:INFO:              kmodes: Not installed
2024-11-10 16:51:03,915:INFO:             mlxtend: Not installed
2024-11-10 16:51:03,915:INFO:       statsforecast: Not installed
2024-11-10 16:51:03,915:INFO:        tune_sklearn: Not installed
2024-11-10 16:51:03,915:INFO:                 ray: Not installed
2024-11-10 16:51:03,915:INFO:            hyperopt: Not installed
2024-11-10 16:51:03,915:INFO:              optuna: Not installed
2024-11-10 16:51:03,915:INFO:               skopt: Not installed
2024-11-10 16:51:03,915:INFO:              mlflow: Not installed
2024-11-10 16:51:03,915:INFO:              gradio: Not installed
2024-11-10 16:51:03,915:INFO:             fastapi: 0.115.4
2024-11-10 16:51:03,915:INFO:             uvicorn: 0.15.0
2024-11-10 16:51:03,915:INFO:              m2cgen: Not installed
2024-11-10 16:51:03,915:INFO:           evidently: Not installed
2024-11-10 16:51:03,915:INFO:               fugue: Not installed
2024-11-10 16:51:03,915:INFO:           streamlit: Not installed
2024-11-10 16:51:03,915:INFO:             prophet: Not installed
2024-11-10 16:51:03,915:INFO:None
2024-11-10 16:51:03,915:INFO:Set up data.
2024-11-10 16:51:03,920:INFO:Set up folding strategy.
2024-11-10 16:51:03,920:INFO:Set up train/test split.
2024-11-10 16:51:03,925:INFO:Set up index.
2024-11-10 16:51:03,925:INFO:Assigning column types.
2024-11-10 16:51:03,928:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-10 16:51:03,958:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 16:51:03,960:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 16:51:04,008:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,009:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,053:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 16:51:04,053:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 16:51:04,074:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,074:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,074:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-10 16:51:04,106:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 16:51:04,125:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,126:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,157:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 16:51:04,176:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,176:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,176:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-10 16:51:04,233:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,233:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,287:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,287:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,288:INFO:Preparing preprocessing pipeline...
2024-11-10 16:51:04,289:INFO:Set up simple imputation.
2024-11-10 16:51:04,292:INFO:Set up encoding of ordinal features.
2024-11-10 16:51:04,300:INFO:Set up encoding of categorical features.
2024-11-10 16:51:04,399:INFO:Finished creating preprocessing pipeline.
2024-11-10 16:51:04,475:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    inclu...
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['GPA'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-10 16:51:04,475:INFO:Creating final display dataframe.
2024-11-10 16:51:04,702:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 25)
5   Transformed train set shape        (1674, 25)
6    Transformed test set shape         (718, 25)
7               Ignore features                 1
8              Numeric features                 3
9          Categorical features                10
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              a2b5
2024-11-10 16:51:04,757:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,757:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,810:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,810:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:51:04,811:INFO:setup() successfully completed in 1.26s...............
2024-11-10 16:51:04,811:INFO:Initializing compare_models()
2024-11-10 16:51:04,811:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-10 16:51:04,811:INFO:Checking exceptions
2024-11-10 16:51:04,814:INFO:Preparing display monitor
2024-11-10 16:51:04,817:INFO:Initializing Logistic Regression
2024-11-10 16:51:04,817:INFO:Total runtime is 1.1642773946126303e-06 minutes
2024-11-10 16:51:04,817:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:04,817:INFO:Initializing create_model()
2024-11-10 16:51:04,817:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:04,817:INFO:Checking exceptions
2024-11-10 16:51:04,817:INFO:Importing libraries
2024-11-10 16:51:04,817:INFO:Copying training dataset
2024-11-10 16:51:04,821:INFO:Defining folds
2024-11-10 16:51:04,821:INFO:Declaring metric variables
2024-11-10 16:51:04,821:INFO:Importing untrained model
2024-11-10 16:51:04,821:INFO:Logistic Regression Imported successfully
2024-11-10 16:51:04,822:INFO:Starting cross validation
2024-11-10 16:51:04,823:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:05,125:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,126:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,146:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,146:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,161:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,161:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,174:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,174:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,187:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,188:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,190:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,190:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,194:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,194:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,198:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,198:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,202:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,202:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:05,204:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:51:05,204:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:51:08,403:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,411:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,442:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,450:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,562:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,569:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,570:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,581:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:51:08,600:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,608:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,610:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,614:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,627:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,630:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,634:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,642:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:08,665:INFO:Calculating mean and std
2024-11-10 16:51:08,666:INFO:Creating metrics dataframe
2024-11-10 16:51:08,671:INFO:Uploading results into container
2024-11-10 16:51:08,672:INFO:Uploading model into container now
2024-11-10 16:51:08,673:INFO:_master_model_container: 1
2024-11-10 16:51:08,673:INFO:_display_container: 2
2024-11-10 16:51:08,674:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-10 16:51:08,674:INFO:create_model() successfully completed......................................
2024-11-10 16:51:08,798:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:08,798:INFO:Creating metrics dataframe
2024-11-10 16:51:08,800:INFO:Initializing K Neighbors Classifier
2024-11-10 16:51:08,800:INFO:Total runtime is 0.06638503472010296 minutes
2024-11-10 16:51:08,800:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:08,800:INFO:Initializing create_model()
2024-11-10 16:51:08,800:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:08,800:INFO:Checking exceptions
2024-11-10 16:51:08,800:INFO:Importing libraries
2024-11-10 16:51:08,800:INFO:Copying training dataset
2024-11-10 16:51:08,805:INFO:Defining folds
2024-11-10 16:51:08,805:INFO:Declaring metric variables
2024-11-10 16:51:08,805:INFO:Importing untrained model
2024-11-10 16:51:08,805:INFO:K Neighbors Classifier Imported successfully
2024-11-10 16:51:08,805:INFO:Starting cross validation
2024-11-10 16:51:08,807:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:10,485:INFO:Calculating mean and std
2024-11-10 16:51:10,486:INFO:Creating metrics dataframe
2024-11-10 16:51:10,488:INFO:Uploading results into container
2024-11-10 16:51:10,488:INFO:Uploading model into container now
2024-11-10 16:51:10,488:INFO:_master_model_container: 2
2024-11-10 16:51:10,488:INFO:_display_container: 2
2024-11-10 16:51:10,489:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-10 16:51:10,489:INFO:create_model() successfully completed......................................
2024-11-10 16:51:10,549:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:10,549:INFO:Creating metrics dataframe
2024-11-10 16:51:10,551:INFO:Initializing Naive Bayes
2024-11-10 16:51:10,551:INFO:Total runtime is 0.09557467301686606 minutes
2024-11-10 16:51:10,551:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:10,551:INFO:Initializing create_model()
2024-11-10 16:51:10,551:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:10,551:INFO:Checking exceptions
2024-11-10 16:51:10,551:INFO:Importing libraries
2024-11-10 16:51:10,551:INFO:Copying training dataset
2024-11-10 16:51:10,555:INFO:Defining folds
2024-11-10 16:51:10,556:INFO:Declaring metric variables
2024-11-10 16:51:10,556:INFO:Importing untrained model
2024-11-10 16:51:10,556:INFO:Naive Bayes Imported successfully
2024-11-10 16:51:10,556:INFO:Starting cross validation
2024-11-10 16:51:10,557:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:10,778:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,802:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,808:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,823:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,827:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,831:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,831:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,837:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,845:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,851:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:10,861:INFO:Calculating mean and std
2024-11-10 16:51:10,861:INFO:Creating metrics dataframe
2024-11-10 16:51:10,863:INFO:Uploading results into container
2024-11-10 16:51:10,863:INFO:Uploading model into container now
2024-11-10 16:51:10,863:INFO:_master_model_container: 3
2024-11-10 16:51:10,863:INFO:_display_container: 2
2024-11-10 16:51:10,863:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-10 16:51:10,863:INFO:create_model() successfully completed......................................
2024-11-10 16:51:10,925:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:10,925:INFO:Creating metrics dataframe
2024-11-10 16:51:10,927:INFO:Initializing Decision Tree Classifier
2024-11-10 16:51:10,928:INFO:Total runtime is 0.10184940894444786 minutes
2024-11-10 16:51:10,928:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:10,928:INFO:Initializing create_model()
2024-11-10 16:51:10,928:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:10,928:INFO:Checking exceptions
2024-11-10 16:51:10,928:INFO:Importing libraries
2024-11-10 16:51:10,928:INFO:Copying training dataset
2024-11-10 16:51:10,932:INFO:Defining folds
2024-11-10 16:51:10,932:INFO:Declaring metric variables
2024-11-10 16:51:10,932:INFO:Importing untrained model
2024-11-10 16:51:10,932:INFO:Decision Tree Classifier Imported successfully
2024-11-10 16:51:10,933:INFO:Starting cross validation
2024-11-10 16:51:10,934:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:11,230:INFO:Calculating mean and std
2024-11-10 16:51:11,231:INFO:Creating metrics dataframe
2024-11-10 16:51:11,232:INFO:Uploading results into container
2024-11-10 16:51:11,233:INFO:Uploading model into container now
2024-11-10 16:51:11,233:INFO:_master_model_container: 4
2024-11-10 16:51:11,233:INFO:_display_container: 2
2024-11-10 16:51:11,233:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-10 16:51:11,233:INFO:create_model() successfully completed......................................
2024-11-10 16:51:11,293:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:11,293:INFO:Creating metrics dataframe
2024-11-10 16:51:11,295:INFO:Initializing SVM - Linear Kernel
2024-11-10 16:51:11,295:INFO:Total runtime is 0.10797707637151084 minutes
2024-11-10 16:51:11,295:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:11,296:INFO:Initializing create_model()
2024-11-10 16:51:11,296:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:11,296:INFO:Checking exceptions
2024-11-10 16:51:11,296:INFO:Importing libraries
2024-11-10 16:51:11,296:INFO:Copying training dataset
2024-11-10 16:51:11,300:INFO:Defining folds
2024-11-10 16:51:11,300:INFO:Declaring metric variables
2024-11-10 16:51:11,300:INFO:Importing untrained model
2024-11-10 16:51:11,300:INFO:SVM - Linear Kernel Imported successfully
2024-11-10 16:51:11,300:INFO:Starting cross validation
2024-11-10 16:51:11,302:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:11,528:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,534:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,546:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,550:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,570:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,574:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,575:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,577:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,581:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,582:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,583:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,587:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,589:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,593:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,597:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,599:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,601:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,614:INFO:Calculating mean and std
2024-11-10 16:51:11,614:INFO:Creating metrics dataframe
2024-11-10 16:51:11,616:INFO:Uploading results into container
2024-11-10 16:51:11,616:INFO:Uploading model into container now
2024-11-10 16:51:11,616:INFO:_master_model_container: 5
2024-11-10 16:51:11,616:INFO:_display_container: 2
2024-11-10 16:51:11,617:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-10 16:51:11,617:INFO:create_model() successfully completed......................................
2024-11-10 16:51:11,670:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:11,670:INFO:Creating metrics dataframe
2024-11-10 16:51:11,673:INFO:Initializing Ridge Classifier
2024-11-10 16:51:11,673:INFO:Total runtime is 0.11426724195480348 minutes
2024-11-10 16:51:11,673:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:11,673:INFO:Initializing create_model()
2024-11-10 16:51:11,673:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:11,673:INFO:Checking exceptions
2024-11-10 16:51:11,673:INFO:Importing libraries
2024-11-10 16:51:11,673:INFO:Copying training dataset
2024-11-10 16:51:11,677:INFO:Defining folds
2024-11-10 16:51:11,677:INFO:Declaring metric variables
2024-11-10 16:51:11,677:INFO:Importing untrained model
2024-11-10 16:51:11,678:INFO:Ridge Classifier Imported successfully
2024-11-10 16:51:11,678:INFO:Starting cross validation
2024-11-10 16:51:11,679:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:11,877:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,878:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,882:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,882:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,897:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,903:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,904:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,904:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,907:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,910:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,910:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,913:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,913:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,918:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,918:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,919:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,921:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:11,922:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,923:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,924:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:11,939:INFO:Calculating mean and std
2024-11-10 16:51:11,941:INFO:Creating metrics dataframe
2024-11-10 16:51:11,945:INFO:Uploading results into container
2024-11-10 16:51:11,946:INFO:Uploading model into container now
2024-11-10 16:51:11,947:INFO:_master_model_container: 6
2024-11-10 16:51:11,948:INFO:_display_container: 2
2024-11-10 16:51:11,948:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-10 16:51:11,949:INFO:create_model() successfully completed......................................
2024-11-10 16:51:12,007:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:12,007:INFO:Creating metrics dataframe
2024-11-10 16:51:12,009:INFO:Initializing Random Forest Classifier
2024-11-10 16:51:12,009:INFO:Total runtime is 0.11987744172414146 minutes
2024-11-10 16:51:12,009:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:12,010:INFO:Initializing create_model()
2024-11-10 16:51:12,010:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:12,010:INFO:Checking exceptions
2024-11-10 16:51:12,010:INFO:Importing libraries
2024-11-10 16:51:12,010:INFO:Copying training dataset
2024-11-10 16:51:12,014:INFO:Defining folds
2024-11-10 16:51:12,014:INFO:Declaring metric variables
2024-11-10 16:51:12,014:INFO:Importing untrained model
2024-11-10 16:51:12,014:INFO:Random Forest Classifier Imported successfully
2024-11-10 16:51:12,014:INFO:Starting cross validation
2024-11-10 16:51:12,016:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:12,655:INFO:Calculating mean and std
2024-11-10 16:51:12,656:INFO:Creating metrics dataframe
2024-11-10 16:51:12,657:INFO:Uploading results into container
2024-11-10 16:51:12,658:INFO:Uploading model into container now
2024-11-10 16:51:12,658:INFO:_master_model_container: 7
2024-11-10 16:51:12,658:INFO:_display_container: 2
2024-11-10 16:51:12,658:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-10 16:51:12,658:INFO:create_model() successfully completed......................................
2024-11-10 16:51:12,712:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:12,712:INFO:Creating metrics dataframe
2024-11-10 16:51:12,714:INFO:Initializing Quadratic Discriminant Analysis
2024-11-10 16:51:12,714:INFO:Total runtime is 0.13162633975346885 minutes
2024-11-10 16:51:12,714:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:12,714:INFO:Initializing create_model()
2024-11-10 16:51:12,714:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:12,715:INFO:Checking exceptions
2024-11-10 16:51:12,715:INFO:Importing libraries
2024-11-10 16:51:12,715:INFO:Copying training dataset
2024-11-10 16:51:12,718:INFO:Defining folds
2024-11-10 16:51:12,719:INFO:Declaring metric variables
2024-11-10 16:51:12,719:INFO:Importing untrained model
2024-11-10 16:51:12,719:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-10 16:51:12,719:INFO:Starting cross validation
2024-11-10 16:51:12,720:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:12,868:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,875:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,882:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,883:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,891:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,905:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,911:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,917:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,924:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,929:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:51:12,948:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,954:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,955:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,965:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,966:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,978:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,979:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,981:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,984:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:12,991:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,002:INFO:Calculating mean and std
2024-11-10 16:51:13,003:INFO:Creating metrics dataframe
2024-11-10 16:51:13,004:INFO:Uploading results into container
2024-11-10 16:51:13,005:INFO:Uploading model into container now
2024-11-10 16:51:13,005:INFO:_master_model_container: 8
2024-11-10 16:51:13,005:INFO:_display_container: 2
2024-11-10 16:51:13,005:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-10 16:51:13,005:INFO:create_model() successfully completed......................................
2024-11-10 16:51:13,060:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:13,060:INFO:Creating metrics dataframe
2024-11-10 16:51:13,062:INFO:Initializing Ada Boost Classifier
2024-11-10 16:51:13,062:INFO:Total runtime is 0.13742423057556155 minutes
2024-11-10 16:51:13,062:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:13,062:INFO:Initializing create_model()
2024-11-10 16:51:13,062:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:13,062:INFO:Checking exceptions
2024-11-10 16:51:13,062:INFO:Importing libraries
2024-11-10 16:51:13,062:INFO:Copying training dataset
2024-11-10 16:51:13,066:INFO:Defining folds
2024-11-10 16:51:13,066:INFO:Declaring metric variables
2024-11-10 16:51:13,067:INFO:Importing untrained model
2024-11-10 16:51:13,067:INFO:Ada Boost Classifier Imported successfully
2024-11-10 16:51:13,067:INFO:Starting cross validation
2024-11-10 16:51:13,068:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:13,199:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,203:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,233:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,235:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,239:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,241:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,242:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,251:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,254:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,254:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:51:13,428:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,443:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,479:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,497:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,501:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,506:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,508:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,514:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,517:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,518:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:13,531:INFO:Calculating mean and std
2024-11-10 16:51:13,532:INFO:Creating metrics dataframe
2024-11-10 16:51:13,534:INFO:Uploading results into container
2024-11-10 16:51:13,534:INFO:Uploading model into container now
2024-11-10 16:51:13,534:INFO:_master_model_container: 9
2024-11-10 16:51:13,534:INFO:_display_container: 2
2024-11-10 16:51:13,534:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-10 16:51:13,534:INFO:create_model() successfully completed......................................
2024-11-10 16:51:13,587:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:13,587:INFO:Creating metrics dataframe
2024-11-10 16:51:13,589:INFO:Initializing Gradient Boosting Classifier
2024-11-10 16:51:13,589:INFO:Total runtime is 0.14620867967605594 minutes
2024-11-10 16:51:13,589:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:13,589:INFO:Initializing create_model()
2024-11-10 16:51:13,589:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:13,589:INFO:Checking exceptions
2024-11-10 16:51:13,589:INFO:Importing libraries
2024-11-10 16:51:13,590:INFO:Copying training dataset
2024-11-10 16:51:13,593:INFO:Defining folds
2024-11-10 16:51:13,593:INFO:Declaring metric variables
2024-11-10 16:51:13,594:INFO:Importing untrained model
2024-11-10 16:51:13,594:INFO:Gradient Boosting Classifier Imported successfully
2024-11-10 16:51:13,594:INFO:Starting cross validation
2024-11-10 16:51:13,595:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:15,000:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,014:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,329:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,343:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,359:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,377:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,401:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,427:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,435:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,436:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:15,454:INFO:Calculating mean and std
2024-11-10 16:51:15,454:INFO:Creating metrics dataframe
2024-11-10 16:51:15,456:INFO:Uploading results into container
2024-11-10 16:51:15,456:INFO:Uploading model into container now
2024-11-10 16:51:15,457:INFO:_master_model_container: 10
2024-11-10 16:51:15,457:INFO:_display_container: 2
2024-11-10 16:51:15,457:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-10 16:51:15,457:INFO:create_model() successfully completed......................................
2024-11-10 16:51:15,510:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:15,510:INFO:Creating metrics dataframe
2024-11-10 16:51:15,513:INFO:Initializing Linear Discriminant Analysis
2024-11-10 16:51:15,513:INFO:Total runtime is 0.1782675623893738 minutes
2024-11-10 16:51:15,513:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:15,513:INFO:Initializing create_model()
2024-11-10 16:51:15,513:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:15,513:INFO:Checking exceptions
2024-11-10 16:51:15,513:INFO:Importing libraries
2024-11-10 16:51:15,513:INFO:Copying training dataset
2024-11-10 16:51:15,517:INFO:Defining folds
2024-11-10 16:51:15,517:INFO:Declaring metric variables
2024-11-10 16:51:15,517:INFO:Importing untrained model
2024-11-10 16:51:15,518:INFO:Linear Discriminant Analysis Imported successfully
2024-11-10 16:51:15,518:INFO:Starting cross validation
2024-11-10 16:51:15,519:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:16,007:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,020:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,020:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,021:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,021:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,021:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,022:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,023:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,032:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,038:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:51:16,043:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:16,050:INFO:Calculating mean and std
2024-11-10 16:51:16,051:INFO:Creating metrics dataframe
2024-11-10 16:51:16,053:INFO:Uploading results into container
2024-11-10 16:51:16,053:INFO:Uploading model into container now
2024-11-10 16:51:16,053:INFO:_master_model_container: 11
2024-11-10 16:51:16,053:INFO:_display_container: 2
2024-11-10 16:51:16,054:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-10 16:51:16,054:INFO:create_model() successfully completed......................................
2024-11-10 16:51:16,110:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:16,110:INFO:Creating metrics dataframe
2024-11-10 16:51:16,112:INFO:Initializing Extra Trees Classifier
2024-11-10 16:51:16,112:INFO:Total runtime is 0.18826510906219485 minutes
2024-11-10 16:51:16,113:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:16,113:INFO:Initializing create_model()
2024-11-10 16:51:16,113:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:16,113:INFO:Checking exceptions
2024-11-10 16:51:16,113:INFO:Importing libraries
2024-11-10 16:51:16,113:INFO:Copying training dataset
2024-11-10 16:51:16,117:INFO:Defining folds
2024-11-10 16:51:16,117:INFO:Declaring metric variables
2024-11-10 16:51:16,117:INFO:Importing untrained model
2024-11-10 16:51:16,117:INFO:Extra Trees Classifier Imported successfully
2024-11-10 16:51:16,118:INFO:Starting cross validation
2024-11-10 16:51:16,119:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:51:16,719:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:51:16,759:INFO:Calculating mean and std
2024-11-10 16:51:16,760:INFO:Creating metrics dataframe
2024-11-10 16:51:16,761:INFO:Uploading results into container
2024-11-10 16:51:16,762:INFO:Uploading model into container now
2024-11-10 16:51:16,762:INFO:_master_model_container: 12
2024-11-10 16:51:16,762:INFO:_display_container: 2
2024-11-10 16:51:16,762:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-10 16:51:16,762:INFO:create_model() successfully completed......................................
2024-11-10 16:51:16,817:INFO:SubProcess create_model() end ==================================
2024-11-10 16:51:16,817:INFO:Creating metrics dataframe
2024-11-10 16:51:16,819:INFO:Initializing Light Gradient Boosting Machine
2024-11-10 16:51:16,819:INFO:Total runtime is 0.20004309415817262 minutes
2024-11-10 16:51:16,819:INFO:SubProcess create_model() called ==================================
2024-11-10 16:51:16,820:INFO:Initializing create_model()
2024-11-10 16:51:16,820:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74755905a410>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7474c82d42e0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:51:16,820:INFO:Checking exceptions
2024-11-10 16:51:16,820:INFO:Importing libraries
2024-11-10 16:51:16,820:INFO:Copying training dataset
2024-11-10 16:51:16,824:INFO:Defining folds
2024-11-10 16:51:16,824:INFO:Declaring metric variables
2024-11-10 16:51:16,824:INFO:Importing untrained model
2024-11-10 16:51:16,824:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-10 16:51:16,825:INFO:Starting cross validation
2024-11-10 16:51:16,826:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:03,248:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 16:56:04,015:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 16:56:04,015:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 16:56:04,015:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 16:56:04,015:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 16:56:04,446:INFO:PyCaret ClassificationExperiment
2024-11-10 16:56:04,446:INFO:Logging name: clf-default-name
2024-11-10 16:56:04,446:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-10 16:56:04,446:INFO:version 3.3.2
2024-11-10 16:56:04,446:INFO:Initializing setup()
2024-11-10 16:56:04,446:INFO:self.USI: 6484
2024-11-10 16:56:04,446:INFO:self._variable_keys: {'idx', 'html_param', 'fold_shuffle_param', 'data', 'fold_generator', '_ml_usecase', 'memory', 'logging_param', 'exp_id', 'fold_groups_param', 'X', 'target_param', 'y_train', 'X_train', 'X_test', 'pipeline', 'USI', 'y', 'fix_imbalance', 'gpu_n_jobs_param', 'is_multiclass', 'log_plots_param', '_available_plots', 'y_test', 'seed', 'n_jobs_param', 'exp_name_log', 'gpu_param'}
2024-11-10 16:56:04,446:INFO:Checking environment
2024-11-10 16:56:04,446:INFO:python_version: 3.10.12
2024-11-10 16:56:04,446:INFO:python_build: ('main', 'Sep 11 2024 15:47:36')
2024-11-10 16:56:04,446:INFO:machine: x86_64
2024-11-10 16:56:04,470:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 16:56:04,471:INFO:Memory: svmem(total=16587202560, available=9337856000, percent=43.7, used=6345515008, free=2143580160, active=9682485248, inactive=3628425216, buffers=220868608, cached=7877238784, shared=555855872, slab=828424192)
2024-11-10 16:56:04,471:INFO:Physical Core: 6
2024-11-10 16:56:04,471:INFO:Logical Core: 12
2024-11-10 16:56:04,471:INFO:Checking libraries
2024-11-10 16:56:04,471:INFO:System:
2024-11-10 16:56:04,471:INFO:    python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
2024-11-10 16:56:04,472:INFO:executable: /usr/bin/python3
2024-11-10 16:56:04,472:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 16:56:04,472:INFO:PyCaret required dependencies:
2024-11-10 16:56:04,485:INFO:                 pip: 22.0.2
2024-11-10 16:56:04,485:INFO:          setuptools: 59.6.0
2024-11-10 16:56:04,485:INFO:             pycaret: 3.3.2
2024-11-10 16:56:04,486:INFO:             IPython: 7.31.1
2024-11-10 16:56:04,486:INFO:          ipywidgets: 8.1.5
2024-11-10 16:56:04,486:INFO:                tqdm: 4.67.0
2024-11-10 16:56:04,486:INFO:               numpy: 1.21.5
2024-11-10 16:56:04,486:INFO:              pandas: 2.1.4
2024-11-10 16:56:04,486:INFO:              jinja2: 3.0.3
2024-11-10 16:56:04,486:INFO:               scipy: 1.8.0
2024-11-10 16:56:04,486:INFO:              joblib: 1.3.2
2024-11-10 16:56:04,486:INFO:             sklearn: 1.4.2
2024-11-10 16:56:04,486:INFO:                pyod: 2.0.2
2024-11-10 16:56:04,486:INFO:            imblearn: 0.12.4
2024-11-10 16:56:04,486:INFO:   category_encoders: 2.6.4
2024-11-10 16:56:04,486:INFO:            lightgbm: 4.5.0
2024-11-10 16:56:04,486:INFO:               numba: 0.60.0
2024-11-10 16:56:04,486:INFO:            requests: 2.25.1
2024-11-10 16:56:04,486:INFO:          matplotlib: 3.5.1
2024-11-10 16:56:04,486:INFO:          scikitplot: 0.3.7
2024-11-10 16:56:04,486:INFO:         yellowbrick: 1.5
2024-11-10 16:56:04,486:INFO:              plotly: 5.24.1
2024-11-10 16:56:04,486:INFO:    plotly-resampler: Not installed
2024-11-10 16:56:04,486:INFO:             kaleido: 0.2.1
2024-11-10 16:56:04,486:INFO:           schemdraw: 0.15
2024-11-10 16:56:04,486:INFO:         statsmodels: 0.14.4
2024-11-10 16:56:04,486:INFO:              sktime: 0.26.0
2024-11-10 16:56:04,487:INFO:               tbats: 1.1.3
2024-11-10 16:56:04,487:INFO:            pmdarima: 2.0.4
2024-11-10 16:56:04,487:INFO:              psutil: 6.1.0
2024-11-10 16:56:04,487:INFO:          markupsafe: 2.0.1
2024-11-10 16:56:04,487:INFO:             pickle5: Not installed
2024-11-10 16:56:04,487:INFO:         cloudpickle: 3.0.0
2024-11-10 16:56:04,487:INFO:         deprecation: 2.1.0
2024-11-10 16:56:04,487:INFO:              xxhash: 3.5.0
2024-11-10 16:56:04,487:INFO:           wurlitzer: 3.1.1
2024-11-10 16:56:04,487:INFO:PyCaret optional dependencies:
2024-11-10 16:56:04,854:INFO:                shap: Not installed
2024-11-10 16:56:04,854:INFO:           interpret: Not installed
2024-11-10 16:56:04,854:INFO:                umap: Not installed
2024-11-10 16:56:04,854:INFO:     ydata_profiling: Not installed
2024-11-10 16:56:04,854:INFO:  explainerdashboard: Not installed
2024-11-10 16:56:04,854:INFO:             autoviz: Not installed
2024-11-10 16:56:04,854:INFO:           fairlearn: Not installed
2024-11-10 16:56:04,854:INFO:          deepchecks: Not installed
2024-11-10 16:56:04,854:INFO:             xgboost: Not installed
2024-11-10 16:56:04,854:INFO:            catboost: Not installed
2024-11-10 16:56:04,854:INFO:              kmodes: Not installed
2024-11-10 16:56:04,854:INFO:             mlxtend: Not installed
2024-11-10 16:56:04,854:INFO:       statsforecast: Not installed
2024-11-10 16:56:04,854:INFO:        tune_sklearn: Not installed
2024-11-10 16:56:04,854:INFO:                 ray: Not installed
2024-11-10 16:56:04,854:INFO:            hyperopt: Not installed
2024-11-10 16:56:04,854:INFO:              optuna: Not installed
2024-11-10 16:56:04,854:INFO:               skopt: Not installed
2024-11-10 16:56:04,854:INFO:              mlflow: Not installed
2024-11-10 16:56:04,855:INFO:              gradio: Not installed
2024-11-10 16:56:04,855:INFO:             fastapi: 0.115.4
2024-11-10 16:56:04,855:INFO:             uvicorn: 0.15.0
2024-11-10 16:56:04,855:INFO:              m2cgen: Not installed
2024-11-10 16:56:04,855:INFO:           evidently: Not installed
2024-11-10 16:56:04,855:INFO:               fugue: Not installed
2024-11-10 16:56:04,855:INFO:           streamlit: Not installed
2024-11-10 16:56:04,855:INFO:             prophet: Not installed
2024-11-10 16:56:04,855:INFO:None
2024-11-10 16:56:04,855:INFO:Set up data.
2024-11-10 16:56:04,860:INFO:Set up folding strategy.
2024-11-10 16:56:04,860:INFO:Set up train/test split.
2024-11-10 16:56:04,864:INFO:Set up index.
2024-11-10 16:56:04,864:INFO:Assigning column types.
2024-11-10 16:56:04,867:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-10 16:56:04,899:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 16:56:04,901:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 16:56:04,922:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:04,922:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:04,953:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 16:56:04,954:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 16:56:04,973:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:04,973:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:04,973:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-10 16:56:05,005:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 16:56:05,024:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,024:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,056:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 16:56:05,075:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,076:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,076:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-10 16:56:05,126:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,126:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,177:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,177:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,178:INFO:Preparing preprocessing pipeline...
2024-11-10 16:56:05,179:INFO:Set up simple imputation.
2024-11-10 16:56:05,182:INFO:Set up encoding of ordinal features.
2024-11-10 16:56:05,189:INFO:Set up encoding of categorical features.
2024-11-10 16:56:05,290:INFO:Finished creating preprocessing pipeline.
2024-11-10 16:56:05,366:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    inclu...
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['GPA'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-10 16:56:05,366:INFO:Creating final display dataframe.
2024-11-10 16:56:05,607:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 25)
5   Transformed train set shape        (1674, 25)
6    Transformed test set shape         (718, 25)
7               Ignore features                 1
8              Numeric features                 3
9          Categorical features                10
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              6484
2024-11-10 16:56:05,663:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,663:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,714:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,714:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 16:56:05,715:INFO:setup() successfully completed in 1.27s...............
2024-11-10 16:56:05,715:INFO:Initializing compare_models()
2024-11-10 16:56:05,715:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-10 16:56:05,715:INFO:Checking exceptions
2024-11-10 16:56:05,719:INFO:Preparing display monitor
2024-11-10 16:56:05,721:INFO:Initializing Logistic Regression
2024-11-10 16:56:05,721:INFO:Total runtime is 1.1642773946126303e-06 minutes
2024-11-10 16:56:05,721:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:05,721:INFO:Initializing create_model()
2024-11-10 16:56:05,721:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:05,721:INFO:Checking exceptions
2024-11-10 16:56:05,721:INFO:Importing libraries
2024-11-10 16:56:05,721:INFO:Copying training dataset
2024-11-10 16:56:05,725:INFO:Defining folds
2024-11-10 16:56:05,725:INFO:Declaring metric variables
2024-11-10 16:56:05,725:INFO:Importing untrained model
2024-11-10 16:56:05,726:INFO:Logistic Regression Imported successfully
2024-11-10 16:56:05,726:INFO:Starting cross validation
2024-11-10 16:56:05,727:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:06,000:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,000:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,010:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,012:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,051:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,053:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,060:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,060:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,090:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,093:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,094:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,096:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,096:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,096:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,101:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,101:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,108:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,108:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:06,108:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 16:56:06,108:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 16:56:07,880:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:07,922:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,000:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,038:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,422:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,457:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,488:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,527:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,563:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,576:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,598:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,610:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,658:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,673:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,681:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,692:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,708:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,729:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,770:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 16:56:08,802:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:08,812:INFO:Calculating mean and std
2024-11-10 16:56:08,813:INFO:Creating metrics dataframe
2024-11-10 16:56:08,815:INFO:Uploading results into container
2024-11-10 16:56:08,815:INFO:Uploading model into container now
2024-11-10 16:56:08,816:INFO:_master_model_container: 1
2024-11-10 16:56:08,816:INFO:_display_container: 2
2024-11-10 16:56:08,816:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-10 16:56:08,816:INFO:create_model() successfully completed......................................
2024-11-10 16:56:08,886:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:08,886:INFO:Creating metrics dataframe
2024-11-10 16:56:08,888:INFO:Initializing K Neighbors Classifier
2024-11-10 16:56:08,888:INFO:Total runtime is 0.05279078086217245 minutes
2024-11-10 16:56:08,888:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:08,888:INFO:Initializing create_model()
2024-11-10 16:56:08,888:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:08,888:INFO:Checking exceptions
2024-11-10 16:56:08,888:INFO:Importing libraries
2024-11-10 16:56:08,888:INFO:Copying training dataset
2024-11-10 16:56:08,892:INFO:Defining folds
2024-11-10 16:56:08,893:INFO:Declaring metric variables
2024-11-10 16:56:08,893:INFO:Importing untrained model
2024-11-10 16:56:08,893:INFO:K Neighbors Classifier Imported successfully
2024-11-10 16:56:08,893:INFO:Starting cross validation
2024-11-10 16:56:08,894:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:10,261:INFO:Calculating mean and std
2024-11-10 16:56:10,262:INFO:Creating metrics dataframe
2024-11-10 16:56:10,264:INFO:Uploading results into container
2024-11-10 16:56:10,264:INFO:Uploading model into container now
2024-11-10 16:56:10,264:INFO:_master_model_container: 2
2024-11-10 16:56:10,265:INFO:_display_container: 2
2024-11-10 16:56:10,265:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-10 16:56:10,265:INFO:create_model() successfully completed......................................
2024-11-10 16:56:10,333:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:10,333:INFO:Creating metrics dataframe
2024-11-10 16:56:10,336:INFO:Initializing Naive Bayes
2024-11-10 16:56:10,336:INFO:Total runtime is 0.07691893180211386 minutes
2024-11-10 16:56:10,336:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:10,336:INFO:Initializing create_model()
2024-11-10 16:56:10,336:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:10,336:INFO:Checking exceptions
2024-11-10 16:56:10,336:INFO:Importing libraries
2024-11-10 16:56:10,336:INFO:Copying training dataset
2024-11-10 16:56:10,340:INFO:Defining folds
2024-11-10 16:56:10,340:INFO:Declaring metric variables
2024-11-10 16:56:10,340:INFO:Importing untrained model
2024-11-10 16:56:10,341:INFO:Naive Bayes Imported successfully
2024-11-10 16:56:10,341:INFO:Starting cross validation
2024-11-10 16:56:10,342:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:10,509:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,520:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,559:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,574:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,574:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,575:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,581:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,584:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,585:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,588:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:10,602:INFO:Calculating mean and std
2024-11-10 16:56:10,602:INFO:Creating metrics dataframe
2024-11-10 16:56:10,604:INFO:Uploading results into container
2024-11-10 16:56:10,604:INFO:Uploading model into container now
2024-11-10 16:56:10,604:INFO:_master_model_container: 3
2024-11-10 16:56:10,604:INFO:_display_container: 2
2024-11-10 16:56:10,605:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-10 16:56:10,605:INFO:create_model() successfully completed......................................
2024-11-10 16:56:10,662:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:10,662:INFO:Creating metrics dataframe
2024-11-10 16:56:10,665:INFO:Initializing Decision Tree Classifier
2024-11-10 16:56:10,665:INFO:Total runtime is 0.08240199883778891 minutes
2024-11-10 16:56:10,665:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:10,665:INFO:Initializing create_model()
2024-11-10 16:56:10,665:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:10,665:INFO:Checking exceptions
2024-11-10 16:56:10,665:INFO:Importing libraries
2024-11-10 16:56:10,665:INFO:Copying training dataset
2024-11-10 16:56:10,669:INFO:Defining folds
2024-11-10 16:56:10,669:INFO:Declaring metric variables
2024-11-10 16:56:10,669:INFO:Importing untrained model
2024-11-10 16:56:10,670:INFO:Decision Tree Classifier Imported successfully
2024-11-10 16:56:10,670:INFO:Starting cross validation
2024-11-10 16:56:10,671:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:10,947:INFO:Calculating mean and std
2024-11-10 16:56:10,948:INFO:Creating metrics dataframe
2024-11-10 16:56:10,949:INFO:Uploading results into container
2024-11-10 16:56:10,950:INFO:Uploading model into container now
2024-11-10 16:56:10,950:INFO:_master_model_container: 4
2024-11-10 16:56:10,950:INFO:_display_container: 2
2024-11-10 16:56:10,950:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-10 16:56:10,951:INFO:create_model() successfully completed......................................
2024-11-10 16:56:11,005:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:11,005:INFO:Creating metrics dataframe
2024-11-10 16:56:11,007:INFO:Initializing SVM - Linear Kernel
2024-11-10 16:56:11,007:INFO:Total runtime is 0.08811140855153403 minutes
2024-11-10 16:56:11,007:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:11,008:INFO:Initializing create_model()
2024-11-10 16:56:11,008:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:11,008:INFO:Checking exceptions
2024-11-10 16:56:11,008:INFO:Importing libraries
2024-11-10 16:56:11,008:INFO:Copying training dataset
2024-11-10 16:56:11,012:INFO:Defining folds
2024-11-10 16:56:11,012:INFO:Declaring metric variables
2024-11-10 16:56:11,012:INFO:Importing untrained model
2024-11-10 16:56:11,012:INFO:SVM - Linear Kernel Imported successfully
2024-11-10 16:56:11,013:INFO:Starting cross validation
2024-11-10 16:56:11,014:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:11,255:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,258:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,262:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,262:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,280:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,282:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,284:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,287:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,293:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,297:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,297:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,301:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,302:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,303:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,304:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,308:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,318:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,322:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,339:INFO:Calculating mean and std
2024-11-10 16:56:11,339:INFO:Creating metrics dataframe
2024-11-10 16:56:11,341:INFO:Uploading results into container
2024-11-10 16:56:11,341:INFO:Uploading model into container now
2024-11-10 16:56:11,341:INFO:_master_model_container: 5
2024-11-10 16:56:11,341:INFO:_display_container: 2
2024-11-10 16:56:11,342:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-10 16:56:11,342:INFO:create_model() successfully completed......................................
2024-11-10 16:56:11,394:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:11,394:INFO:Creating metrics dataframe
2024-11-10 16:56:11,396:INFO:Initializing Ridge Classifier
2024-11-10 16:56:11,396:INFO:Total runtime is 0.09459397395451864 minutes
2024-11-10 16:56:11,396:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:11,397:INFO:Initializing create_model()
2024-11-10 16:56:11,397:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:11,397:INFO:Checking exceptions
2024-11-10 16:56:11,397:INFO:Importing libraries
2024-11-10 16:56:11,397:INFO:Copying training dataset
2024-11-10 16:56:11,401:INFO:Defining folds
2024-11-10 16:56:11,401:INFO:Declaring metric variables
2024-11-10 16:56:11,401:INFO:Importing untrained model
2024-11-10 16:56:11,401:INFO:Ridge Classifier Imported successfully
2024-11-10 16:56:11,401:INFO:Starting cross validation
2024-11-10 16:56:11,403:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:11,581:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,585:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,600:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,602:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,604:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,608:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,615:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,619:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,619:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,625:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,625:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,629:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,632:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,634:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,638:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,638:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,638:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,639:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:11,641:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,645:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:11,653:INFO:Calculating mean and std
2024-11-10 16:56:11,654:INFO:Creating metrics dataframe
2024-11-10 16:56:11,655:INFO:Uploading results into container
2024-11-10 16:56:11,656:INFO:Uploading model into container now
2024-11-10 16:56:11,656:INFO:_master_model_container: 6
2024-11-10 16:56:11,656:INFO:_display_container: 2
2024-11-10 16:56:11,656:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-10 16:56:11,656:INFO:create_model() successfully completed......................................
2024-11-10 16:56:11,719:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:11,719:INFO:Creating metrics dataframe
2024-11-10 16:56:11,721:INFO:Initializing Random Forest Classifier
2024-11-10 16:56:11,721:INFO:Total runtime is 0.10000498692194622 minutes
2024-11-10 16:56:11,721:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:11,721:INFO:Initializing create_model()
2024-11-10 16:56:11,721:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:11,721:INFO:Checking exceptions
2024-11-10 16:56:11,721:INFO:Importing libraries
2024-11-10 16:56:11,721:INFO:Copying training dataset
2024-11-10 16:56:11,725:INFO:Defining folds
2024-11-10 16:56:11,725:INFO:Declaring metric variables
2024-11-10 16:56:11,726:INFO:Importing untrained model
2024-11-10 16:56:11,726:INFO:Random Forest Classifier Imported successfully
2024-11-10 16:56:11,726:INFO:Starting cross validation
2024-11-10 16:56:11,728:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:12,391:INFO:Calculating mean and std
2024-11-10 16:56:12,393:INFO:Creating metrics dataframe
2024-11-10 16:56:12,394:INFO:Uploading results into container
2024-11-10 16:56:12,395:INFO:Uploading model into container now
2024-11-10 16:56:12,395:INFO:_master_model_container: 7
2024-11-10 16:56:12,395:INFO:_display_container: 2
2024-11-10 16:56:12,395:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-10 16:56:12,395:INFO:create_model() successfully completed......................................
2024-11-10 16:56:12,449:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:12,449:INFO:Creating metrics dataframe
2024-11-10 16:56:12,452:INFO:Initializing Quadratic Discriminant Analysis
2024-11-10 16:56:12,452:INFO:Total runtime is 0.1121837615966797 minutes
2024-11-10 16:56:12,452:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:12,452:INFO:Initializing create_model()
2024-11-10 16:56:12,452:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:12,452:INFO:Checking exceptions
2024-11-10 16:56:12,452:INFO:Importing libraries
2024-11-10 16:56:12,452:INFO:Copying training dataset
2024-11-10 16:56:12,456:INFO:Defining folds
2024-11-10 16:56:12,456:INFO:Declaring metric variables
2024-11-10 16:56:12,456:INFO:Importing untrained model
2024-11-10 16:56:12,456:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-10 16:56:12,457:INFO:Starting cross validation
2024-11-10 16:56:12,458:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:12,601:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,607:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,614:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,630:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,630:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,633:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,637:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,643:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,643:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,648:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 16:56:12,655:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,659:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,681:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,693:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,693:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,695:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,697:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,699:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,702:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,707:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:12,721:INFO:Calculating mean and std
2024-11-10 16:56:12,722:INFO:Creating metrics dataframe
2024-11-10 16:56:12,724:INFO:Uploading results into container
2024-11-10 16:56:12,724:INFO:Uploading model into container now
2024-11-10 16:56:12,724:INFO:_master_model_container: 8
2024-11-10 16:56:12,724:INFO:_display_container: 2
2024-11-10 16:56:12,725:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-10 16:56:12,725:INFO:create_model() successfully completed......................................
2024-11-10 16:56:12,782:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:12,782:INFO:Creating metrics dataframe
2024-11-10 16:56:12,785:INFO:Initializing Ada Boost Classifier
2024-11-10 16:56:12,785:INFO:Total runtime is 0.11773356199264527 minutes
2024-11-10 16:56:12,785:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:12,785:INFO:Initializing create_model()
2024-11-10 16:56:12,785:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:12,785:INFO:Checking exceptions
2024-11-10 16:56:12,785:INFO:Importing libraries
2024-11-10 16:56:12,785:INFO:Copying training dataset
2024-11-10 16:56:12,789:INFO:Defining folds
2024-11-10 16:56:12,789:INFO:Declaring metric variables
2024-11-10 16:56:12,789:INFO:Importing untrained model
2024-11-10 16:56:12,789:INFO:Ada Boost Classifier Imported successfully
2024-11-10 16:56:12,789:INFO:Starting cross validation
2024-11-10 16:56:12,791:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:12,908:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,913:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,953:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,955:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,959:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,964:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,968:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,972:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,972:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:12,975:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 16:56:13,094:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,128:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,163:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,185:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,187:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,193:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,203:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,205:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,214:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,220:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:13,232:INFO:Calculating mean and std
2024-11-10 16:56:13,233:INFO:Creating metrics dataframe
2024-11-10 16:56:13,234:INFO:Uploading results into container
2024-11-10 16:56:13,235:INFO:Uploading model into container now
2024-11-10 16:56:13,235:INFO:_master_model_container: 9
2024-11-10 16:56:13,235:INFO:_display_container: 2
2024-11-10 16:56:13,235:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-10 16:56:13,235:INFO:create_model() successfully completed......................................
2024-11-10 16:56:13,289:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:13,289:INFO:Creating metrics dataframe
2024-11-10 16:56:13,291:INFO:Initializing Gradient Boosting Classifier
2024-11-10 16:56:13,291:INFO:Total runtime is 0.12617320617039998 minutes
2024-11-10 16:56:13,291:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:13,291:INFO:Initializing create_model()
2024-11-10 16:56:13,291:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:13,291:INFO:Checking exceptions
2024-11-10 16:56:13,291:INFO:Importing libraries
2024-11-10 16:56:13,291:INFO:Copying training dataset
2024-11-10 16:56:13,295:INFO:Defining folds
2024-11-10 16:56:13,295:INFO:Declaring metric variables
2024-11-10 16:56:13,296:INFO:Importing untrained model
2024-11-10 16:56:13,296:INFO:Gradient Boosting Classifier Imported successfully
2024-11-10 16:56:13,296:INFO:Starting cross validation
2024-11-10 16:56:13,297:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:14,731:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:14,776:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,005:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,097:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,149:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,166:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,166:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,175:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,184:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,201:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,217:INFO:Calculating mean and std
2024-11-10 16:56:15,217:INFO:Creating metrics dataframe
2024-11-10 16:56:15,219:INFO:Uploading results into container
2024-11-10 16:56:15,219:INFO:Uploading model into container now
2024-11-10 16:56:15,220:INFO:_master_model_container: 10
2024-11-10 16:56:15,220:INFO:_display_container: 2
2024-11-10 16:56:15,220:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-10 16:56:15,220:INFO:create_model() successfully completed......................................
2024-11-10 16:56:15,278:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:15,278:INFO:Creating metrics dataframe
2024-11-10 16:56:15,280:INFO:Initializing Linear Discriminant Analysis
2024-11-10 16:56:15,280:INFO:Total runtime is 0.15932650963465372 minutes
2024-11-10 16:56:15,280:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:15,280:INFO:Initializing create_model()
2024-11-10 16:56:15,280:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:15,281:INFO:Checking exceptions
2024-11-10 16:56:15,281:INFO:Importing libraries
2024-11-10 16:56:15,281:INFO:Copying training dataset
2024-11-10 16:56:15,285:INFO:Defining folds
2024-11-10 16:56:15,285:INFO:Declaring metric variables
2024-11-10 16:56:15,285:INFO:Importing untrained model
2024-11-10 16:56:15,285:INFO:Linear Discriminant Analysis Imported successfully
2024-11-10 16:56:15,285:INFO:Starting cross validation
2024-11-10 16:56:15,287:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:15,472:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,477:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,515:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,520:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,520:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,523:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,525:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,530:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,533:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,536:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 16:56:15,539:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:15,546:INFO:Calculating mean and std
2024-11-10 16:56:15,547:INFO:Creating metrics dataframe
2024-11-10 16:56:15,548:INFO:Uploading results into container
2024-11-10 16:56:15,549:INFO:Uploading model into container now
2024-11-10 16:56:15,549:INFO:_master_model_container: 11
2024-11-10 16:56:15,549:INFO:_display_container: 2
2024-11-10 16:56:15,549:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-10 16:56:15,549:INFO:create_model() successfully completed......................................
2024-11-10 16:56:15,608:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:15,608:INFO:Creating metrics dataframe
2024-11-10 16:56:15,610:INFO:Initializing Extra Trees Classifier
2024-11-10 16:56:15,610:INFO:Total runtime is 0.16482215325037639 minutes
2024-11-10 16:56:15,610:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:15,610:INFO:Initializing create_model()
2024-11-10 16:56:15,610:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:15,610:INFO:Checking exceptions
2024-11-10 16:56:15,610:INFO:Importing libraries
2024-11-10 16:56:15,610:INFO:Copying training dataset
2024-11-10 16:56:15,614:INFO:Defining folds
2024-11-10 16:56:15,614:INFO:Declaring metric variables
2024-11-10 16:56:15,614:INFO:Importing untrained model
2024-11-10 16:56:15,615:INFO:Extra Trees Classifier Imported successfully
2024-11-10 16:56:15,615:INFO:Starting cross validation
2024-11-10 16:56:15,616:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 16:56:16,256:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 16:56:16,262:INFO:Calculating mean and std
2024-11-10 16:56:16,262:INFO:Creating metrics dataframe
2024-11-10 16:56:16,264:INFO:Uploading results into container
2024-11-10 16:56:16,264:INFO:Uploading model into container now
2024-11-10 16:56:16,265:INFO:_master_model_container: 12
2024-11-10 16:56:16,265:INFO:_display_container: 2
2024-11-10 16:56:16,265:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-10 16:56:16,265:INFO:create_model() successfully completed......................................
2024-11-10 16:56:16,322:INFO:SubProcess create_model() end ==================================
2024-11-10 16:56:16,322:INFO:Creating metrics dataframe
2024-11-10 16:56:16,324:INFO:Initializing Light Gradient Boosting Machine
2024-11-10 16:56:16,324:INFO:Total runtime is 0.17672826449076334 minutes
2024-11-10 16:56:16,324:INFO:SubProcess create_model() called ==================================
2024-11-10 16:56:16,325:INFO:Initializing create_model()
2024-11-10 16:56:16,325:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7681418189d0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7680b0803940>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 16:56:16,325:INFO:Checking exceptions
2024-11-10 16:56:16,325:INFO:Importing libraries
2024-11-10 16:56:16,325:INFO:Copying training dataset
2024-11-10 16:56:16,329:INFO:Defining folds
2024-11-10 16:56:16,329:INFO:Declaring metric variables
2024-11-10 16:56:16,329:INFO:Importing untrained model
2024-11-10 16:56:16,329:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-10 16:56:16,329:INFO:Starting cross validation
2024-11-10 16:56:16,331:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:31,383:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 17:02:32,516:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 17:02:32,516:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 17:02:32,516:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 17:02:32,516:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 17:02:33,302:INFO:PyCaret ClassificationExperiment
2024-11-10 17:02:33,302:INFO:Logging name: clf-default-name
2024-11-10 17:02:33,302:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-10 17:02:33,302:INFO:version 3.3.2
2024-11-10 17:02:33,302:INFO:Initializing setup()
2024-11-10 17:02:33,302:INFO:self.USI: bad9
2024-11-10 17:02:33,302:INFO:self._variable_keys: {'memory', '_ml_usecase', 'gpu_n_jobs_param', 'log_plots_param', 'exp_name_log', 'target_param', 'fold_shuffle_param', 'X_test', 'html_param', 'exp_id', 'fold_generator', 'X_train', 'y_train', 'X', 'is_multiclass', 'fix_imbalance', 'y_test', 'seed', 'y', 'fold_groups_param', 'data', 'gpu_param', 'idx', 'USI', 'pipeline', 'n_jobs_param', '_available_plots', 'logging_param'}
2024-11-10 17:02:33,303:INFO:Checking environment
2024-11-10 17:02:33,303:INFO:python_version: 3.10.12
2024-11-10 17:02:33,303:INFO:python_build: ('main', 'Sep 11 2024 15:47:36')
2024-11-10 17:02:33,303:INFO:machine: x86_64
2024-11-10 17:02:33,304:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 17:02:33,304:INFO:Memory: svmem(total=16587202560, available=10429648896, percent=37.1, used=5190127616, free=3459383296, active=8808169472, inactive=3126202368, buffers=187658240, cached=7750033408, shared=619429888, slab=823451648)
2024-11-10 17:02:33,305:INFO:Physical Core: 6
2024-11-10 17:02:33,305:INFO:Logical Core: 12
2024-11-10 17:02:33,305:INFO:Checking libraries
2024-11-10 17:02:33,305:INFO:System:
2024-11-10 17:02:33,305:INFO:    python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
2024-11-10 17:02:33,305:INFO:executable: /usr/bin/python3
2024-11-10 17:02:33,305:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 17:02:33,305:INFO:PyCaret required dependencies:
2024-11-10 17:02:33,322:INFO:                 pip: 22.0.2
2024-11-10 17:02:33,322:INFO:          setuptools: 59.6.0
2024-11-10 17:02:33,323:INFO:             pycaret: 3.3.2
2024-11-10 17:02:33,323:INFO:             IPython: 7.31.1
2024-11-10 17:02:33,323:INFO:          ipywidgets: 8.1.5
2024-11-10 17:02:33,323:INFO:                tqdm: 4.67.0
2024-11-10 17:02:33,323:INFO:               numpy: 1.21.5
2024-11-10 17:02:33,323:INFO:              pandas: 2.1.4
2024-11-10 17:02:33,323:INFO:              jinja2: 3.0.3
2024-11-10 17:02:33,323:INFO:               scipy: 1.8.0
2024-11-10 17:02:33,323:INFO:              joblib: 1.3.2
2024-11-10 17:02:33,323:INFO:             sklearn: 1.4.2
2024-11-10 17:02:33,323:INFO:                pyod: 2.0.2
2024-11-10 17:02:33,323:INFO:            imblearn: 0.12.4
2024-11-10 17:02:33,323:INFO:   category_encoders: 2.6.4
2024-11-10 17:02:33,323:INFO:            lightgbm: 4.5.0
2024-11-10 17:02:33,323:INFO:               numba: 0.60.0
2024-11-10 17:02:33,323:INFO:            requests: 2.25.1
2024-11-10 17:02:33,323:INFO:          matplotlib: 3.5.1
2024-11-10 17:02:33,323:INFO:          scikitplot: 0.3.7
2024-11-10 17:02:33,323:INFO:         yellowbrick: 1.5
2024-11-10 17:02:33,323:INFO:              plotly: 5.24.1
2024-11-10 17:02:33,323:INFO:    plotly-resampler: Not installed
2024-11-10 17:02:33,323:INFO:             kaleido: 0.2.1
2024-11-10 17:02:33,323:INFO:           schemdraw: 0.15
2024-11-10 17:02:33,323:INFO:         statsmodels: 0.14.4
2024-11-10 17:02:33,323:INFO:              sktime: 0.26.0
2024-11-10 17:02:33,323:INFO:               tbats: 1.1.3
2024-11-10 17:02:33,323:INFO:            pmdarima: 2.0.4
2024-11-10 17:02:33,323:INFO:              psutil: 6.1.0
2024-11-10 17:02:33,323:INFO:          markupsafe: 2.0.1
2024-11-10 17:02:33,323:INFO:             pickle5: Not installed
2024-11-10 17:02:33,323:INFO:         cloudpickle: 3.0.0
2024-11-10 17:02:33,323:INFO:         deprecation: 2.1.0
2024-11-10 17:02:33,323:INFO:              xxhash: 3.5.0
2024-11-10 17:02:33,323:INFO:           wurlitzer: 3.1.1
2024-11-10 17:02:33,323:INFO:PyCaret optional dependencies:
2024-11-10 17:02:33,674:INFO:                shap: Not installed
2024-11-10 17:02:33,674:INFO:           interpret: Not installed
2024-11-10 17:02:33,674:INFO:                umap: Not installed
2024-11-10 17:02:33,674:INFO:     ydata_profiling: Not installed
2024-11-10 17:02:33,674:INFO:  explainerdashboard: Not installed
2024-11-10 17:02:33,674:INFO:             autoviz: Not installed
2024-11-10 17:02:33,674:INFO:           fairlearn: Not installed
2024-11-10 17:02:33,674:INFO:          deepchecks: Not installed
2024-11-10 17:02:33,674:INFO:             xgboost: Not installed
2024-11-10 17:02:33,674:INFO:            catboost: Not installed
2024-11-10 17:02:33,674:INFO:              kmodes: Not installed
2024-11-10 17:02:33,674:INFO:             mlxtend: Not installed
2024-11-10 17:02:33,674:INFO:       statsforecast: Not installed
2024-11-10 17:02:33,674:INFO:        tune_sklearn: Not installed
2024-11-10 17:02:33,674:INFO:                 ray: Not installed
2024-11-10 17:02:33,674:INFO:            hyperopt: Not installed
2024-11-10 17:02:33,675:INFO:              optuna: Not installed
2024-11-10 17:02:33,675:INFO:               skopt: Not installed
2024-11-10 17:02:33,675:INFO:              mlflow: Not installed
2024-11-10 17:02:33,675:INFO:              gradio: Not installed
2024-11-10 17:02:33,675:INFO:             fastapi: 0.115.4
2024-11-10 17:02:33,675:INFO:             uvicorn: 0.15.0
2024-11-10 17:02:33,675:INFO:              m2cgen: Not installed
2024-11-10 17:02:33,675:INFO:           evidently: Not installed
2024-11-10 17:02:33,675:INFO:               fugue: Not installed
2024-11-10 17:02:33,675:INFO:           streamlit: Not installed
2024-11-10 17:02:33,675:INFO:             prophet: Not installed
2024-11-10 17:02:33,675:INFO:None
2024-11-10 17:02:33,675:INFO:Set up data.
2024-11-10 17:02:33,680:INFO:Set up folding strategy.
2024-11-10 17:02:33,680:INFO:Set up train/test split.
2024-11-10 17:02:33,684:INFO:Set up index.
2024-11-10 17:02:33,685:INFO:Assigning column types.
2024-11-10 17:02:33,688:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-10 17:02:33,718:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 17:02:33,720:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 17:02:33,747:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,747:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,777:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 17:02:33,778:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 17:02:33,797:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,797:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,797:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-10 17:02:33,827:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 17:02:33,846:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,846:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,878:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 17:02:33,897:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,897:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,897:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-10 17:02:33,946:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,947:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,997:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,997:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:33,998:INFO:Preparing preprocessing pipeline...
2024-11-10 17:02:33,999:INFO:Set up simple imputation.
2024-11-10 17:02:34,002:INFO:Set up encoding of ordinal features.
2024-11-10 17:02:34,009:INFO:Set up encoding of categorical features.
2024-11-10 17:02:34,108:INFO:Finished creating preprocessing pipeline.
2024-11-10 17:02:34,206:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'StudyTimeWeekly',
                                             'Absences'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None,
                                    inclu...
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              return_df=True,
                                                              use_cat_names=True,
                                                              verbose=0))),
                ('rest_encoding',
                 TransformerWrapper(exclude=None, include=['GPA'],
                                    transformer=TargetEncoder(cols=[],
                                                              drop_invariant=False,
                                                              handle_missing='return_nan',
                                                              handle_unknown='value',
                                                              hierarchy=None,
                                                              min_samples_leaf=20,
                                                              return_df=True,
                                                              smoothing=10,
                                                              verbose=0)))],
         verbose=False)
2024-11-10 17:02:34,206:INFO:Creating final display dataframe.
2024-11-10 17:02:34,455:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 25)
5   Transformed train set shape        (1674, 25)
6    Transformed test set shape         (718, 25)
7               Ignore features                 1
8              Numeric features                 3
9          Categorical features                10
10                   Preprocess              True
11              Imputation type            simple
12           Numeric imputation              mean
13       Categorical imputation              mode
14     Maximum one-hot encoding                25
15              Encoding method              None
16               Fold Generator   StratifiedKFold
17                  Fold Number                10
18                     CPU Jobs                -1
19                      Use GPU             False
20               Log Experiment             False
21              Experiment Name  clf-default-name
22                          USI              bad9
2024-11-10 17:02:34,508:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:34,508:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:34,556:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:34,557:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:02:34,558:INFO:setup() successfully completed in 1.26s...............
2024-11-10 17:02:34,558:INFO:Initializing compare_models()
2024-11-10 17:02:34,558:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-10 17:02:34,558:INFO:Checking exceptions
2024-11-10 17:02:34,561:INFO:Preparing display monitor
2024-11-10 17:02:34,563:INFO:Initializing Logistic Regression
2024-11-10 17:02:34,563:INFO:Total runtime is 9.099642435709636e-07 minutes
2024-11-10 17:02:34,563:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:34,563:INFO:Initializing create_model()
2024-11-10 17:02:34,563:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:34,563:INFO:Checking exceptions
2024-11-10 17:02:34,563:INFO:Importing libraries
2024-11-10 17:02:34,563:INFO:Copying training dataset
2024-11-10 17:02:34,567:INFO:Defining folds
2024-11-10 17:02:34,567:INFO:Declaring metric variables
2024-11-10 17:02:34,567:INFO:Importing untrained model
2024-11-10 17:02:34,568:INFO:Logistic Regression Imported successfully
2024-11-10 17:02:34,568:INFO:Starting cross validation
2024-11-10 17:02:34,569:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:34,807:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,807:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,872:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,873:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,884:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,884:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,909:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,909:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,939:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,939:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,948:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,948:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,951:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,951:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,958:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,960:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,967:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,968:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:34,978:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:02:34,978:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:02:36,661:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:36,705:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:36,944:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,001:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,077:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,113:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,232:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,266:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,354:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,382:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,389:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,418:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,465:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,489:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,503:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,512:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,519:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:02:37,522:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,547:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,558:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:37,576:INFO:Calculating mean and std
2024-11-10 17:02:37,577:INFO:Creating metrics dataframe
2024-11-10 17:02:37,580:INFO:Uploading results into container
2024-11-10 17:02:37,580:INFO:Uploading model into container now
2024-11-10 17:02:37,581:INFO:_master_model_container: 1
2024-11-10 17:02:37,581:INFO:_display_container: 2
2024-11-10 17:02:37,582:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-10 17:02:37,582:INFO:create_model() successfully completed......................................
2024-11-10 17:02:37,658:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:37,658:INFO:Creating metrics dataframe
2024-11-10 17:02:37,660:INFO:Initializing K Neighbors Classifier
2024-11-10 17:02:37,660:INFO:Total runtime is 0.051622168223063154 minutes
2024-11-10 17:02:37,660:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:37,660:INFO:Initializing create_model()
2024-11-10 17:02:37,660:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:37,660:INFO:Checking exceptions
2024-11-10 17:02:37,660:INFO:Importing libraries
2024-11-10 17:02:37,660:INFO:Copying training dataset
2024-11-10 17:02:37,664:INFO:Defining folds
2024-11-10 17:02:37,664:INFO:Declaring metric variables
2024-11-10 17:02:37,665:INFO:Importing untrained model
2024-11-10 17:02:37,665:INFO:K Neighbors Classifier Imported successfully
2024-11-10 17:02:37,665:INFO:Starting cross validation
2024-11-10 17:02:37,666:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:39,059:INFO:Calculating mean and std
2024-11-10 17:02:39,059:INFO:Creating metrics dataframe
2024-11-10 17:02:39,061:INFO:Uploading results into container
2024-11-10 17:02:39,061:INFO:Uploading model into container now
2024-11-10 17:02:39,061:INFO:_master_model_container: 2
2024-11-10 17:02:39,061:INFO:_display_container: 2
2024-11-10 17:02:39,062:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-10 17:02:39,062:INFO:create_model() successfully completed......................................
2024-11-10 17:02:39,121:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:39,121:INFO:Creating metrics dataframe
2024-11-10 17:02:39,123:INFO:Initializing Naive Bayes
2024-11-10 17:02:39,123:INFO:Total runtime is 0.07600797017415364 minutes
2024-11-10 17:02:39,123:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:39,123:INFO:Initializing create_model()
2024-11-10 17:02:39,123:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:39,124:INFO:Checking exceptions
2024-11-10 17:02:39,124:INFO:Importing libraries
2024-11-10 17:02:39,124:INFO:Copying training dataset
2024-11-10 17:02:39,127:INFO:Defining folds
2024-11-10 17:02:39,127:INFO:Declaring metric variables
2024-11-10 17:02:39,128:INFO:Importing untrained model
2024-11-10 17:02:39,128:INFO:Naive Bayes Imported successfully
2024-11-10 17:02:39,128:INFO:Starting cross validation
2024-11-10 17:02:39,129:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:39,345:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,363:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,367:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,369:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,370:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,373:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,375:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,380:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,383:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,383:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:39,390:INFO:Calculating mean and std
2024-11-10 17:02:39,390:INFO:Creating metrics dataframe
2024-11-10 17:02:39,397:INFO:Uploading results into container
2024-11-10 17:02:39,399:INFO:Uploading model into container now
2024-11-10 17:02:39,400:INFO:_master_model_container: 3
2024-11-10 17:02:39,400:INFO:_display_container: 2
2024-11-10 17:02:39,401:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-10 17:02:39,401:INFO:create_model() successfully completed......................................
2024-11-10 17:02:39,458:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:39,458:INFO:Creating metrics dataframe
2024-11-10 17:02:39,461:INFO:Initializing Decision Tree Classifier
2024-11-10 17:02:39,461:INFO:Total runtime is 0.08163119157155355 minutes
2024-11-10 17:02:39,461:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:39,461:INFO:Initializing create_model()
2024-11-10 17:02:39,461:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:39,461:INFO:Checking exceptions
2024-11-10 17:02:39,461:INFO:Importing libraries
2024-11-10 17:02:39,461:INFO:Copying training dataset
2024-11-10 17:02:39,465:INFO:Defining folds
2024-11-10 17:02:39,465:INFO:Declaring metric variables
2024-11-10 17:02:39,465:INFO:Importing untrained model
2024-11-10 17:02:39,465:INFO:Decision Tree Classifier Imported successfully
2024-11-10 17:02:39,466:INFO:Starting cross validation
2024-11-10 17:02:39,468:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:39,728:INFO:Calculating mean and std
2024-11-10 17:02:39,729:INFO:Creating metrics dataframe
2024-11-10 17:02:39,731:INFO:Uploading results into container
2024-11-10 17:02:39,733:INFO:Uploading model into container now
2024-11-10 17:02:39,734:INFO:_master_model_container: 4
2024-11-10 17:02:39,735:INFO:_display_container: 2
2024-11-10 17:02:39,736:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-10 17:02:39,736:INFO:create_model() successfully completed......................................
2024-11-10 17:02:39,792:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:39,792:INFO:Creating metrics dataframe
2024-11-10 17:02:39,794:INFO:Initializing SVM - Linear Kernel
2024-11-10 17:02:39,794:INFO:Total runtime is 0.08719358046849568 minutes
2024-11-10 17:02:39,794:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:39,795:INFO:Initializing create_model()
2024-11-10 17:02:39,795:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:39,795:INFO:Checking exceptions
2024-11-10 17:02:39,795:INFO:Importing libraries
2024-11-10 17:02:39,795:INFO:Copying training dataset
2024-11-10 17:02:39,799:INFO:Defining folds
2024-11-10 17:02:39,799:INFO:Declaring metric variables
2024-11-10 17:02:39,799:INFO:Importing untrained model
2024-11-10 17:02:39,799:INFO:SVM - Linear Kernel Imported successfully
2024-11-10 17:02:39,799:INFO:Starting cross validation
2024-11-10 17:02:39,802:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:40,047:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,052:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,091:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,094:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,095:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,097:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,099:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,099:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,100:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,101:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,102:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,102:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,104:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,105:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,106:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,107:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,109:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,115:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,119:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,128:INFO:Calculating mean and std
2024-11-10 17:02:40,128:INFO:Creating metrics dataframe
2024-11-10 17:02:40,130:INFO:Uploading results into container
2024-11-10 17:02:40,130:INFO:Uploading model into container now
2024-11-10 17:02:40,131:INFO:_master_model_container: 5
2024-11-10 17:02:40,131:INFO:_display_container: 2
2024-11-10 17:02:40,131:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-10 17:02:40,131:INFO:create_model() successfully completed......................................
2024-11-10 17:02:40,184:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:40,184:INFO:Creating metrics dataframe
2024-11-10 17:02:40,186:INFO:Initializing Ridge Classifier
2024-11-10 17:02:40,186:INFO:Total runtime is 0.09371902545293172 minutes
2024-11-10 17:02:40,186:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:40,186:INFO:Initializing create_model()
2024-11-10 17:02:40,186:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:40,186:INFO:Checking exceptions
2024-11-10 17:02:40,186:INFO:Importing libraries
2024-11-10 17:02:40,186:INFO:Copying training dataset
2024-11-10 17:02:40,190:INFO:Defining folds
2024-11-10 17:02:40,190:INFO:Declaring metric variables
2024-11-10 17:02:40,190:INFO:Importing untrained model
2024-11-10 17:02:40,191:INFO:Ridge Classifier Imported successfully
2024-11-10 17:02:40,191:INFO:Starting cross validation
2024-11-10 17:02:40,192:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:40,352:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,357:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,366:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,371:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,402:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,408:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,411:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,412:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,414:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,414:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,415:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,418:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,418:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,421:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,427:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,427:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,430:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:40,430:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,431:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,433:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:40,441:INFO:Calculating mean and std
2024-11-10 17:02:40,441:INFO:Creating metrics dataframe
2024-11-10 17:02:40,443:INFO:Uploading results into container
2024-11-10 17:02:40,443:INFO:Uploading model into container now
2024-11-10 17:02:40,443:INFO:_master_model_container: 6
2024-11-10 17:02:40,443:INFO:_display_container: 2
2024-11-10 17:02:40,444:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-10 17:02:40,444:INFO:create_model() successfully completed......................................
2024-11-10 17:02:40,495:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:40,495:INFO:Creating metrics dataframe
2024-11-10 17:02:40,498:INFO:Initializing Random Forest Classifier
2024-11-10 17:02:40,498:INFO:Total runtime is 0.09891424973805744 minutes
2024-11-10 17:02:40,498:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:40,498:INFO:Initializing create_model()
2024-11-10 17:02:40,498:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:40,498:INFO:Checking exceptions
2024-11-10 17:02:40,498:INFO:Importing libraries
2024-11-10 17:02:40,498:INFO:Copying training dataset
2024-11-10 17:02:40,502:INFO:Defining folds
2024-11-10 17:02:40,502:INFO:Declaring metric variables
2024-11-10 17:02:40,502:INFO:Importing untrained model
2024-11-10 17:02:40,502:INFO:Random Forest Classifier Imported successfully
2024-11-10 17:02:40,502:INFO:Starting cross validation
2024-11-10 17:02:40,504:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:41,143:INFO:Calculating mean and std
2024-11-10 17:02:41,144:INFO:Creating metrics dataframe
2024-11-10 17:02:41,146:INFO:Uploading results into container
2024-11-10 17:02:41,146:INFO:Uploading model into container now
2024-11-10 17:02:41,146:INFO:_master_model_container: 7
2024-11-10 17:02:41,146:INFO:_display_container: 2
2024-11-10 17:02:41,147:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-10 17:02:41,147:INFO:create_model() successfully completed......................................
2024-11-10 17:02:41,200:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:41,200:INFO:Creating metrics dataframe
2024-11-10 17:02:41,202:INFO:Initializing Quadratic Discriminant Analysis
2024-11-10 17:02:41,202:INFO:Total runtime is 0.11065608263015746 minutes
2024-11-10 17:02:41,202:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:41,202:INFO:Initializing create_model()
2024-11-10 17:02:41,202:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:41,203:INFO:Checking exceptions
2024-11-10 17:02:41,203:INFO:Importing libraries
2024-11-10 17:02:41,203:INFO:Copying training dataset
2024-11-10 17:02:41,207:INFO:Defining folds
2024-11-10 17:02:41,207:INFO:Declaring metric variables
2024-11-10 17:02:41,207:INFO:Importing untrained model
2024-11-10 17:02:41,207:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-10 17:02:41,207:INFO:Starting cross validation
2024-11-10 17:02:41,209:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:41,321:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,332:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,362:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,369:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,371:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,375:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,383:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,383:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,384:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,390:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,393:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/discriminant_analysis.py:935: UserWarning: Variables are collinear
  warnings.warn("Variables are collinear")

2024-11-10 17:02:41,400:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,406:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,430:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,434:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,435:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,441:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,441:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,442:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,449:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,467:INFO:Calculating mean and std
2024-11-10 17:02:41,467:INFO:Creating metrics dataframe
2024-11-10 17:02:41,469:INFO:Uploading results into container
2024-11-10 17:02:41,469:INFO:Uploading model into container now
2024-11-10 17:02:41,469:INFO:_master_model_container: 8
2024-11-10 17:02:41,469:INFO:_display_container: 2
2024-11-10 17:02:41,469:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-10 17:02:41,469:INFO:create_model() successfully completed......................................
2024-11-10 17:02:41,523:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:41,523:INFO:Creating metrics dataframe
2024-11-10 17:02:41,525:INFO:Initializing Ada Boost Classifier
2024-11-10 17:02:41,526:INFO:Total runtime is 0.11604739030202228 minutes
2024-11-10 17:02:41,526:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:41,526:INFO:Initializing create_model()
2024-11-10 17:02:41,526:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:41,526:INFO:Checking exceptions
2024-11-10 17:02:41,526:INFO:Importing libraries
2024-11-10 17:02:41,526:INFO:Copying training dataset
2024-11-10 17:02:41,530:INFO:Defining folds
2024-11-10 17:02:41,530:INFO:Declaring metric variables
2024-11-10 17:02:41,530:INFO:Importing untrained model
2024-11-10 17:02:41,530:INFO:Ada Boost Classifier Imported successfully
2024-11-10 17:02:41,531:INFO:Starting cross validation
2024-11-10 17:02:41,532:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:41,657:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,665:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,687:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,692:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,697:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,700:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,704:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,710:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,717:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,722:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:02:41,832:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,837:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,911:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,916:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,934:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,947:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,949:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,952:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,956:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,968:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:41,985:INFO:Calculating mean and std
2024-11-10 17:02:41,985:INFO:Creating metrics dataframe
2024-11-10 17:02:41,987:INFO:Uploading results into container
2024-11-10 17:02:41,987:INFO:Uploading model into container now
2024-11-10 17:02:41,987:INFO:_master_model_container: 9
2024-11-10 17:02:41,987:INFO:_display_container: 2
2024-11-10 17:02:41,988:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-10 17:02:41,988:INFO:create_model() successfully completed......................................
2024-11-10 17:02:42,040:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:42,040:INFO:Creating metrics dataframe
2024-11-10 17:02:42,042:INFO:Initializing Gradient Boosting Classifier
2024-11-10 17:02:42,042:INFO:Total runtime is 0.12465627988179523 minutes
2024-11-10 17:02:42,042:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:42,042:INFO:Initializing create_model()
2024-11-10 17:02:42,042:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:42,042:INFO:Checking exceptions
2024-11-10 17:02:42,043:INFO:Importing libraries
2024-11-10 17:02:42,043:INFO:Copying training dataset
2024-11-10 17:02:42,046:INFO:Defining folds
2024-11-10 17:02:42,046:INFO:Declaring metric variables
2024-11-10 17:02:42,047:INFO:Importing untrained model
2024-11-10 17:02:42,047:INFO:Gradient Boosting Classifier Imported successfully
2024-11-10 17:02:42,047:INFO:Starting cross validation
2024-11-10 17:02:42,048:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:43,402:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,465:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,752:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,771:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,791:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,796:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,870:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,877:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,877:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,880:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:43,892:INFO:Calculating mean and std
2024-11-10 17:02:43,892:INFO:Creating metrics dataframe
2024-11-10 17:02:43,894:INFO:Uploading results into container
2024-11-10 17:02:43,894:INFO:Uploading model into container now
2024-11-10 17:02:43,894:INFO:_master_model_container: 10
2024-11-10 17:02:43,894:INFO:_display_container: 2
2024-11-10 17:02:43,895:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-10 17:02:43,895:INFO:create_model() successfully completed......................................
2024-11-10 17:02:43,949:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:43,949:INFO:Creating metrics dataframe
2024-11-10 17:02:43,951:INFO:Initializing Linear Discriminant Analysis
2024-11-10 17:02:43,951:INFO:Total runtime is 0.15646838347117104 minutes
2024-11-10 17:02:43,951:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:43,951:INFO:Initializing create_model()
2024-11-10 17:02:43,951:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:43,951:INFO:Checking exceptions
2024-11-10 17:02:43,951:INFO:Importing libraries
2024-11-10 17:02:43,951:INFO:Copying training dataset
2024-11-10 17:02:43,955:INFO:Defining folds
2024-11-10 17:02:43,955:INFO:Declaring metric variables
2024-11-10 17:02:43,955:INFO:Importing untrained model
2024-11-10 17:02:43,956:INFO:Linear Discriminant Analysis Imported successfully
2024-11-10 17:02:43,956:INFO:Starting cross validation
2024-11-10 17:02:43,957:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:44,133:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,147:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,178:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,183:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,185:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,186:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,187:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,192:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,199:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,200:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:02:44,203:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:44,217:INFO:Calculating mean and std
2024-11-10 17:02:44,217:INFO:Creating metrics dataframe
2024-11-10 17:02:44,219:INFO:Uploading results into container
2024-11-10 17:02:44,219:INFO:Uploading model into container now
2024-11-10 17:02:44,220:INFO:_master_model_container: 11
2024-11-10 17:02:44,220:INFO:_display_container: 2
2024-11-10 17:02:44,220:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-10 17:02:44,220:INFO:create_model() successfully completed......................................
2024-11-10 17:02:44,272:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:44,272:INFO:Creating metrics dataframe
2024-11-10 17:02:44,274:INFO:Initializing Extra Trees Classifier
2024-11-10 17:02:44,274:INFO:Total runtime is 0.16185246706008907 minutes
2024-11-10 17:02:44,274:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:44,274:INFO:Initializing create_model()
2024-11-10 17:02:44,274:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:44,274:INFO:Checking exceptions
2024-11-10 17:02:44,274:INFO:Importing libraries
2024-11-10 17:02:44,274:INFO:Copying training dataset
2024-11-10 17:02:44,278:INFO:Defining folds
2024-11-10 17:02:44,278:INFO:Declaring metric variables
2024-11-10 17:02:44,278:INFO:Importing untrained model
2024-11-10 17:02:44,279:INFO:Extra Trees Classifier Imported successfully
2024-11-10 17:02:44,279:INFO:Starting cross validation
2024-11-10 17:02:44,280:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:02:44,857:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:02:44,884:INFO:Calculating mean and std
2024-11-10 17:02:44,885:INFO:Creating metrics dataframe
2024-11-10 17:02:44,886:INFO:Uploading results into container
2024-11-10 17:02:44,887:INFO:Uploading model into container now
2024-11-10 17:02:44,887:INFO:_master_model_container: 12
2024-11-10 17:02:44,887:INFO:_display_container: 2
2024-11-10 17:02:44,887:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-10 17:02:44,887:INFO:create_model() successfully completed......................................
2024-11-10 17:02:44,946:INFO:SubProcess create_model() end ==================================
2024-11-10 17:02:44,947:INFO:Creating metrics dataframe
2024-11-10 17:02:44,949:INFO:Initializing Light Gradient Boosting Machine
2024-11-10 17:02:44,949:INFO:Total runtime is 0.1730995655059814 minutes
2024-11-10 17:02:44,949:INFO:SubProcess create_model() called ==================================
2024-11-10 17:02:44,949:INFO:Initializing create_model()
2024-11-10 17:02:44,949:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x74c0335d20e0>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x74bfa25137c0>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:02:44,949:INFO:Checking exceptions
2024-11-10 17:02:44,949:INFO:Importing libraries
2024-11-10 17:02:44,949:INFO:Copying training dataset
2024-11-10 17:02:44,953:INFO:Defining folds
2024-11-10 17:02:44,953:INFO:Declaring metric variables
2024-11-10 17:02:44,953:INFO:Importing untrained model
2024-11-10 17:02:44,953:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-10 17:02:44,954:INFO:Starting cross validation
2024-11-10 17:02:44,955:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:50,456:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 17:33:51,333:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 17:33:51,333:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 17:33:51,333:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 17:33:51,333:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 17:33:51,749:INFO:PyCaret ClassificationExperiment
2024-11-10 17:33:51,749:INFO:Logging name: clf-default-name
2024-11-10 17:33:51,749:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-10 17:33:51,749:INFO:version 3.3.2
2024-11-10 17:33:51,750:INFO:Initializing setup()
2024-11-10 17:33:51,750:INFO:self.USI: 093a
2024-11-10 17:33:51,750:INFO:self._variable_keys: {'y_test', 'idx', 'gpu_param', 'seed', '_ml_usecase', 'y', 'gpu_n_jobs_param', 'X', 'USI', 'fix_imbalance', 'y_train', 'X_train', 'is_multiclass', 'fold_groups_param', 'memory', 'html_param', 'log_plots_param', 'exp_id', 'exp_name_log', 'pipeline', 'target_param', 'X_test', 'data', 'logging_param', 'fold_generator', 'n_jobs_param', 'fold_shuffle_param', '_available_plots'}
2024-11-10 17:33:51,750:INFO:Checking environment
2024-11-10 17:33:51,750:INFO:python_version: 3.10.12
2024-11-10 17:33:51,750:INFO:python_build: ('main', 'Sep 11 2024 15:47:36')
2024-11-10 17:33:51,750:INFO:machine: x86_64
2024-11-10 17:33:51,751:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 17:33:51,751:INFO:Memory: svmem(total=16587202560, available=10069229568, percent=39.3, used=5548867584, free=2815057920, active=9178161152, inactive=3418771456, buffers=193421312, cached=8029855744, shared=621076480, slab=817168384)
2024-11-10 17:33:51,751:INFO:Physical Core: 6
2024-11-10 17:33:51,751:INFO:Logical Core: 12
2024-11-10 17:33:51,751:INFO:Checking libraries
2024-11-10 17:33:51,751:INFO:System:
2024-11-10 17:33:51,751:INFO:    python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
2024-11-10 17:33:51,751:INFO:executable: /usr/bin/python3
2024-11-10 17:33:51,751:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 17:33:51,752:INFO:PyCaret required dependencies:
2024-11-10 17:33:51,764:INFO:                 pip: 22.0.2
2024-11-10 17:33:51,764:INFO:          setuptools: 59.6.0
2024-11-10 17:33:51,765:INFO:             pycaret: 3.3.2
2024-11-10 17:33:51,765:INFO:             IPython: 7.31.1
2024-11-10 17:33:51,765:INFO:          ipywidgets: 8.1.5
2024-11-10 17:33:51,765:INFO:                tqdm: 4.67.0
2024-11-10 17:33:51,765:INFO:               numpy: 1.21.5
2024-11-10 17:33:51,765:INFO:              pandas: 2.1.4
2024-11-10 17:33:51,765:INFO:              jinja2: 3.0.3
2024-11-10 17:33:51,765:INFO:               scipy: 1.8.0
2024-11-10 17:33:51,765:INFO:              joblib: 1.3.2
2024-11-10 17:33:51,765:INFO:             sklearn: 1.4.2
2024-11-10 17:33:51,765:INFO:                pyod: 2.0.2
2024-11-10 17:33:51,765:INFO:            imblearn: 0.12.4
2024-11-10 17:33:51,765:INFO:   category_encoders: 2.6.4
2024-11-10 17:33:51,765:INFO:            lightgbm: 4.5.0
2024-11-10 17:33:51,765:INFO:               numba: 0.60.0
2024-11-10 17:33:51,765:INFO:            requests: 2.25.1
2024-11-10 17:33:51,765:INFO:          matplotlib: 3.5.1
2024-11-10 17:33:51,765:INFO:          scikitplot: 0.3.7
2024-11-10 17:33:51,765:INFO:         yellowbrick: 1.5
2024-11-10 17:33:51,765:INFO:              plotly: 5.24.1
2024-11-10 17:33:51,765:INFO:    plotly-resampler: Not installed
2024-11-10 17:33:51,765:INFO:             kaleido: 0.2.1
2024-11-10 17:33:51,765:INFO:           schemdraw: 0.15
2024-11-10 17:33:51,765:INFO:         statsmodels: 0.14.4
2024-11-10 17:33:51,765:INFO:              sktime: 0.26.0
2024-11-10 17:33:51,765:INFO:               tbats: 1.1.3
2024-11-10 17:33:51,765:INFO:            pmdarima: 2.0.4
2024-11-10 17:33:51,765:INFO:              psutil: 6.1.0
2024-11-10 17:33:51,765:INFO:          markupsafe: 2.0.1
2024-11-10 17:33:51,766:INFO:             pickle5: Not installed
2024-11-10 17:33:51,766:INFO:         cloudpickle: 3.0.0
2024-11-10 17:33:51,766:INFO:         deprecation: 2.1.0
2024-11-10 17:33:51,766:INFO:              xxhash: 3.5.0
2024-11-10 17:33:51,766:INFO:           wurlitzer: 3.1.1
2024-11-10 17:33:51,766:INFO:PyCaret optional dependencies:
2024-11-10 17:33:52,122:INFO:                shap: Not installed
2024-11-10 17:33:52,122:INFO:           interpret: Not installed
2024-11-10 17:33:52,122:INFO:                umap: Not installed
2024-11-10 17:33:52,122:INFO:     ydata_profiling: Not installed
2024-11-10 17:33:52,122:INFO:  explainerdashboard: Not installed
2024-11-10 17:33:52,122:INFO:             autoviz: Not installed
2024-11-10 17:33:52,122:INFO:           fairlearn: Not installed
2024-11-10 17:33:52,122:INFO:          deepchecks: Not installed
2024-11-10 17:33:52,122:INFO:             xgboost: Not installed
2024-11-10 17:33:52,122:INFO:            catboost: Not installed
2024-11-10 17:33:52,122:INFO:              kmodes: Not installed
2024-11-10 17:33:52,122:INFO:             mlxtend: Not installed
2024-11-10 17:33:52,122:INFO:       statsforecast: Not installed
2024-11-10 17:33:52,122:INFO:        tune_sklearn: Not installed
2024-11-10 17:33:52,123:INFO:                 ray: Not installed
2024-11-10 17:33:52,123:INFO:            hyperopt: Not installed
2024-11-10 17:33:52,123:INFO:              optuna: Not installed
2024-11-10 17:33:52,123:INFO:               skopt: Not installed
2024-11-10 17:33:52,123:INFO:              mlflow: Not installed
2024-11-10 17:33:52,123:INFO:              gradio: Not installed
2024-11-10 17:33:52,123:INFO:             fastapi: 0.115.4
2024-11-10 17:33:52,123:INFO:             uvicorn: 0.15.0
2024-11-10 17:33:52,123:INFO:              m2cgen: Not installed
2024-11-10 17:33:52,123:INFO:           evidently: Not installed
2024-11-10 17:33:52,123:INFO:               fugue: Not installed
2024-11-10 17:33:52,123:INFO:           streamlit: Not installed
2024-11-10 17:33:52,123:INFO:             prophet: Not installed
2024-11-10 17:33:52,123:INFO:None
2024-11-10 17:33:52,123:INFO:Set up data.
2024-11-10 17:33:52,128:INFO:Set up folding strategy.
2024-11-10 17:33:52,128:INFO:Set up train/test split.
2024-11-10 17:33:52,132:INFO:Set up index.
2024-11-10 17:33:52,132:INFO:Assigning column types.
2024-11-10 17:33:52,136:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-10 17:33:52,167:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 17:33:52,169:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 17:33:52,190:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,190:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,228:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 17:33:52,229:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 17:33:52,248:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,248:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,248:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-10 17:33:52,279:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 17:33:52,298:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,298:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,330:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 17:33:52,349:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,349:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,349:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-10 17:33:52,400:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,400:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,452:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,453:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,454:INFO:Preparing preprocessing pipeline...
2024-11-10 17:33:52,454:INFO:Set up simple imputation.
2024-11-10 17:33:52,471:INFO:Finished creating preprocessing pipeline.
2024-11-10 17:33:52,474:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-11-10 17:33:52,474:INFO:Creating final display dataframe.
2024-11-10 17:33:52,525:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 14)
5   Transformed train set shape        (1674, 14)
6    Transformed test set shape         (718, 14)
7               Ignore features                 1
8              Numeric features                13
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13               Fold Generator   StratifiedKFold
14                  Fold Number                10
15                     CPU Jobs                -1
16                      Use GPU             False
17               Log Experiment             False
18              Experiment Name  clf-default-name
19                          USI              093a
2024-11-10 17:33:52,581:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,581:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,630:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,631:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 17:33:52,632:INFO:setup() successfully completed in 0.88s...............
2024-11-10 17:33:52,632:INFO:Initializing compare_models()
2024-11-10 17:33:52,632:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-10 17:33:52,632:INFO:Checking exceptions
2024-11-10 17:33:52,635:INFO:Preparing display monitor
2024-11-10 17:33:52,637:INFO:Initializing Logistic Regression
2024-11-10 17:33:52,637:INFO:Total runtime is 8.62280527750651e-07 minutes
2024-11-10 17:33:52,637:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:52,637:INFO:Initializing create_model()
2024-11-10 17:33:52,637:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:52,637:INFO:Checking exceptions
2024-11-10 17:33:52,637:INFO:Importing libraries
2024-11-10 17:33:52,637:INFO:Copying training dataset
2024-11-10 17:33:52,641:INFO:Defining folds
2024-11-10 17:33:52,641:INFO:Declaring metric variables
2024-11-10 17:33:52,641:INFO:Importing untrained model
2024-11-10 17:33:52,641:INFO:Logistic Regression Imported successfully
2024-11-10 17:33:52,641:INFO:Starting cross validation
2024-11-10 17:33:52,642:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:52,931:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:52,932:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:52,936:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:52,936:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:52,950:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:52,950:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:52,955:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:52,955:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:52,973:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:52,973:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:52,995:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:52,995:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:53,030:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:53,030:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:53,065:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:53,066:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:53,086:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:53,086:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:53,094:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 17:33:53,097:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 17:33:55,100:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,108:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,145:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,152:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,438:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,445:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,486:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,493:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,512:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,520:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,533:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,539:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,544:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,547:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,597:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,607:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,624:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,631:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,650:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 17:33:55,657:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:55,676:INFO:Calculating mean and std
2024-11-10 17:33:55,676:INFO:Creating metrics dataframe
2024-11-10 17:33:55,679:INFO:Uploading results into container
2024-11-10 17:33:55,679:INFO:Uploading model into container now
2024-11-10 17:33:55,679:INFO:_master_model_container: 1
2024-11-10 17:33:55,679:INFO:_display_container: 2
2024-11-10 17:33:55,681:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-10 17:33:55,682:INFO:create_model() successfully completed......................................
2024-11-10 17:33:55,745:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:55,745:INFO:Creating metrics dataframe
2024-11-10 17:33:55,747:INFO:Initializing K Neighbors Classifier
2024-11-10 17:33:55,747:INFO:Total runtime is 0.05183891852696737 minutes
2024-11-10 17:33:55,747:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:55,747:INFO:Initializing create_model()
2024-11-10 17:33:55,747:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:55,747:INFO:Checking exceptions
2024-11-10 17:33:55,747:INFO:Importing libraries
2024-11-10 17:33:55,747:INFO:Copying training dataset
2024-11-10 17:33:55,751:INFO:Defining folds
2024-11-10 17:33:55,751:INFO:Declaring metric variables
2024-11-10 17:33:55,751:INFO:Importing untrained model
2024-11-10 17:33:55,752:INFO:K Neighbors Classifier Imported successfully
2024-11-10 17:33:55,752:INFO:Starting cross validation
2024-11-10 17:33:55,752:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:56,924:INFO:Calculating mean and std
2024-11-10 17:33:56,925:INFO:Creating metrics dataframe
2024-11-10 17:33:56,926:INFO:Uploading results into container
2024-11-10 17:33:56,927:INFO:Uploading model into container now
2024-11-10 17:33:56,928:INFO:_master_model_container: 2
2024-11-10 17:33:56,928:INFO:_display_container: 2
2024-11-10 17:33:56,928:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-10 17:33:56,928:INFO:create_model() successfully completed......................................
2024-11-10 17:33:56,990:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:56,991:INFO:Creating metrics dataframe
2024-11-10 17:33:56,993:INFO:Initializing Naive Bayes
2024-11-10 17:33:56,993:INFO:Total runtime is 0.07260440985361735 minutes
2024-11-10 17:33:56,993:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:56,993:INFO:Initializing create_model()
2024-11-10 17:33:56,993:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:56,993:INFO:Checking exceptions
2024-11-10 17:33:56,993:INFO:Importing libraries
2024-11-10 17:33:56,993:INFO:Copying training dataset
2024-11-10 17:33:56,997:INFO:Defining folds
2024-11-10 17:33:56,997:INFO:Declaring metric variables
2024-11-10 17:33:56,998:INFO:Importing untrained model
2024-11-10 17:33:56,998:INFO:Naive Bayes Imported successfully
2024-11-10 17:33:56,998:INFO:Starting cross validation
2024-11-10 17:33:56,999:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:57,043:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,047:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,047:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,047:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,049:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,051:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,053:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,058:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,066:INFO:Calculating mean and std
2024-11-10 17:33:57,067:INFO:Creating metrics dataframe
2024-11-10 17:33:57,068:INFO:Uploading results into container
2024-11-10 17:33:57,068:INFO:Uploading model into container now
2024-11-10 17:33:57,069:INFO:_master_model_container: 3
2024-11-10 17:33:57,069:INFO:_display_container: 2
2024-11-10 17:33:57,069:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-10 17:33:57,069:INFO:create_model() successfully completed......................................
2024-11-10 17:33:57,120:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:57,120:INFO:Creating metrics dataframe
2024-11-10 17:33:57,122:INFO:Initializing Decision Tree Classifier
2024-11-10 17:33:57,123:INFO:Total runtime is 0.07476573387781779 minutes
2024-11-10 17:33:57,123:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:57,123:INFO:Initializing create_model()
2024-11-10 17:33:57,123:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:57,123:INFO:Checking exceptions
2024-11-10 17:33:57,123:INFO:Importing libraries
2024-11-10 17:33:57,123:INFO:Copying training dataset
2024-11-10 17:33:57,127:INFO:Defining folds
2024-11-10 17:33:57,127:INFO:Declaring metric variables
2024-11-10 17:33:57,127:INFO:Importing untrained model
2024-11-10 17:33:57,127:INFO:Decision Tree Classifier Imported successfully
2024-11-10 17:33:57,127:INFO:Starting cross validation
2024-11-10 17:33:57,129:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:57,224:INFO:Calculating mean and std
2024-11-10 17:33:57,225:INFO:Creating metrics dataframe
2024-11-10 17:33:57,227:INFO:Uploading results into container
2024-11-10 17:33:57,227:INFO:Uploading model into container now
2024-11-10 17:33:57,227:INFO:_master_model_container: 4
2024-11-10 17:33:57,227:INFO:_display_container: 2
2024-11-10 17:33:57,228:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-10 17:33:57,228:INFO:create_model() successfully completed......................................
2024-11-10 17:33:57,280:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:57,280:INFO:Creating metrics dataframe
2024-11-10 17:33:57,282:INFO:Initializing SVM - Linear Kernel
2024-11-10 17:33:57,282:INFO:Total runtime is 0.07741928497950236 minutes
2024-11-10 17:33:57,282:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:57,282:INFO:Initializing create_model()
2024-11-10 17:33:57,282:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:57,282:INFO:Checking exceptions
2024-11-10 17:33:57,282:INFO:Importing libraries
2024-11-10 17:33:57,282:INFO:Copying training dataset
2024-11-10 17:33:57,286:INFO:Defining folds
2024-11-10 17:33:57,286:INFO:Declaring metric variables
2024-11-10 17:33:57,286:INFO:Importing untrained model
2024-11-10 17:33:57,286:INFO:SVM - Linear Kernel Imported successfully
2024-11-10 17:33:57,287:INFO:Starting cross validation
2024-11-10 17:33:57,287:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:57,402:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,403:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,403:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,403:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,403:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,403:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,403:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,403:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,404:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,404:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,406:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,409:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,409:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,410:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,410:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,410:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,411:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,412:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,410:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,430:INFO:Calculating mean and std
2024-11-10 17:33:57,431:INFO:Creating metrics dataframe
2024-11-10 17:33:57,432:INFO:Uploading results into container
2024-11-10 17:33:57,432:INFO:Uploading model into container now
2024-11-10 17:33:57,433:INFO:_master_model_container: 5
2024-11-10 17:33:57,433:INFO:_display_container: 2
2024-11-10 17:33:57,433:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-10 17:33:57,433:INFO:create_model() successfully completed......................................
2024-11-10 17:33:57,490:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:57,490:INFO:Creating metrics dataframe
2024-11-10 17:33:57,492:INFO:Initializing Ridge Classifier
2024-11-10 17:33:57,492:INFO:Total runtime is 0.0809208075205485 minutes
2024-11-10 17:33:57,492:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:57,492:INFO:Initializing create_model()
2024-11-10 17:33:57,492:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:57,492:INFO:Checking exceptions
2024-11-10 17:33:57,492:INFO:Importing libraries
2024-11-10 17:33:57,492:INFO:Copying training dataset
2024-11-10 17:33:57,496:INFO:Defining folds
2024-11-10 17:33:57,496:INFO:Declaring metric variables
2024-11-10 17:33:57,497:INFO:Importing untrained model
2024-11-10 17:33:57,497:INFO:Ridge Classifier Imported successfully
2024-11-10 17:33:57,497:INFO:Starting cross validation
2024-11-10 17:33:57,498:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:57,562:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,562:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,565:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,566:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:57,566:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,567:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,571:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,572:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,572:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:33:57,586:INFO:Calculating mean and std
2024-11-10 17:33:57,587:INFO:Creating metrics dataframe
2024-11-10 17:33:57,588:INFO:Uploading results into container
2024-11-10 17:33:57,589:INFO:Uploading model into container now
2024-11-10 17:33:57,589:INFO:_master_model_container: 6
2024-11-10 17:33:57,589:INFO:_display_container: 2
2024-11-10 17:33:57,589:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-10 17:33:57,589:INFO:create_model() successfully completed......................................
2024-11-10 17:33:57,641:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:57,641:INFO:Creating metrics dataframe
2024-11-10 17:33:57,643:INFO:Initializing Random Forest Classifier
2024-11-10 17:33:57,643:INFO:Total runtime is 0.08344516754150391 minutes
2024-11-10 17:33:57,643:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:57,644:INFO:Initializing create_model()
2024-11-10 17:33:57,644:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:57,644:INFO:Checking exceptions
2024-11-10 17:33:57,644:INFO:Importing libraries
2024-11-10 17:33:57,644:INFO:Copying training dataset
2024-11-10 17:33:57,648:INFO:Defining folds
2024-11-10 17:33:57,648:INFO:Declaring metric variables
2024-11-10 17:33:57,648:INFO:Importing untrained model
2024-11-10 17:33:57,648:INFO:Random Forest Classifier Imported successfully
2024-11-10 17:33:57,648:INFO:Starting cross validation
2024-11-10 17:33:57,649:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:58,095:INFO:Calculating mean and std
2024-11-10 17:33:58,095:INFO:Creating metrics dataframe
2024-11-10 17:33:58,097:INFO:Uploading results into container
2024-11-10 17:33:58,097:INFO:Uploading model into container now
2024-11-10 17:33:58,097:INFO:_master_model_container: 7
2024-11-10 17:33:58,097:INFO:_display_container: 2
2024-11-10 17:33:58,097:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-10 17:33:58,098:INFO:create_model() successfully completed......................................
2024-11-10 17:33:58,151:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:58,152:INFO:Creating metrics dataframe
2024-11-10 17:33:58,155:INFO:Initializing Quadratic Discriminant Analysis
2024-11-10 17:33:58,155:INFO:Total runtime is 0.09197036425272624 minutes
2024-11-10 17:33:58,155:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:58,155:INFO:Initializing create_model()
2024-11-10 17:33:58,155:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:58,155:INFO:Checking exceptions
2024-11-10 17:33:58,155:INFO:Importing libraries
2024-11-10 17:33:58,155:INFO:Copying training dataset
2024-11-10 17:33:58,159:INFO:Defining folds
2024-11-10 17:33:58,159:INFO:Declaring metric variables
2024-11-10 17:33:58,159:INFO:Importing untrained model
2024-11-10 17:33:58,160:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-10 17:33:58,160:INFO:Starting cross validation
2024-11-10 17:33:58,160:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:58,190:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,191:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,193:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,194:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,197:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,200:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,202:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,205:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,206:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,207:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,218:INFO:Calculating mean and std
2024-11-10 17:33:58,219:INFO:Creating metrics dataframe
2024-11-10 17:33:58,220:INFO:Uploading results into container
2024-11-10 17:33:58,221:INFO:Uploading model into container now
2024-11-10 17:33:58,221:INFO:_master_model_container: 8
2024-11-10 17:33:58,221:INFO:_display_container: 2
2024-11-10 17:33:58,221:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-10 17:33:58,221:INFO:create_model() successfully completed......................................
2024-11-10 17:33:58,273:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:58,273:INFO:Creating metrics dataframe
2024-11-10 17:33:58,276:INFO:Initializing Ada Boost Classifier
2024-11-10 17:33:58,276:INFO:Total runtime is 0.09398340781529745 minutes
2024-11-10 17:33:58,276:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:58,276:INFO:Initializing create_model()
2024-11-10 17:33:58,276:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:58,276:INFO:Checking exceptions
2024-11-10 17:33:58,276:INFO:Importing libraries
2024-11-10 17:33:58,276:INFO:Copying training dataset
2024-11-10 17:33:58,280:INFO:Defining folds
2024-11-10 17:33:58,280:INFO:Declaring metric variables
2024-11-10 17:33:58,280:INFO:Importing untrained model
2024-11-10 17:33:58,281:INFO:Ada Boost Classifier Imported successfully
2024-11-10 17:33:58,281:INFO:Starting cross validation
2024-11-10 17:33:58,281:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:58,299:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,301:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,302:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,303:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,306:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,308:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,309:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,309:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,314:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,315:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 17:33:58,423:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,442:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,480:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,482:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,482:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,485:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,487:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,488:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,488:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,491:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:58,511:INFO:Calculating mean and std
2024-11-10 17:33:58,512:INFO:Creating metrics dataframe
2024-11-10 17:33:58,513:INFO:Uploading results into container
2024-11-10 17:33:58,513:INFO:Uploading model into container now
2024-11-10 17:33:58,514:INFO:_master_model_container: 9
2024-11-10 17:33:58,514:INFO:_display_container: 2
2024-11-10 17:33:58,514:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-10 17:33:58,514:INFO:create_model() successfully completed......................................
2024-11-10 17:33:58,568:INFO:SubProcess create_model() end ==================================
2024-11-10 17:33:58,568:INFO:Creating metrics dataframe
2024-11-10 17:33:58,570:INFO:Initializing Gradient Boosting Classifier
2024-11-10 17:33:58,570:INFO:Total runtime is 0.09889554977416992 minutes
2024-11-10 17:33:58,570:INFO:SubProcess create_model() called ==================================
2024-11-10 17:33:58,571:INFO:Initializing create_model()
2024-11-10 17:33:58,571:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:33:58,571:INFO:Checking exceptions
2024-11-10 17:33:58,571:INFO:Importing libraries
2024-11-10 17:33:58,571:INFO:Copying training dataset
2024-11-10 17:33:58,575:INFO:Defining folds
2024-11-10 17:33:58,575:INFO:Declaring metric variables
2024-11-10 17:33:58,575:INFO:Importing untrained model
2024-11-10 17:33:58,575:INFO:Gradient Boosting Classifier Imported successfully
2024-11-10 17:33:58,575:INFO:Starting cross validation
2024-11-10 17:33:58,576:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:33:59,788:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:33:59,811:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,057:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,071:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,077:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,097:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,147:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,150:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,164:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,182:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,196:INFO:Calculating mean and std
2024-11-10 17:34:00,197:INFO:Creating metrics dataframe
2024-11-10 17:34:00,198:INFO:Uploading results into container
2024-11-10 17:34:00,198:INFO:Uploading model into container now
2024-11-10 17:34:00,199:INFO:_master_model_container: 10
2024-11-10 17:34:00,199:INFO:_display_container: 2
2024-11-10 17:34:00,199:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-10 17:34:00,199:INFO:create_model() successfully completed......................................
2024-11-10 17:34:00,251:INFO:SubProcess create_model() end ==================================
2024-11-10 17:34:00,251:INFO:Creating metrics dataframe
2024-11-10 17:34:00,253:INFO:Initializing Linear Discriminant Analysis
2024-11-10 17:34:00,253:INFO:Total runtime is 0.12694061994552613 minutes
2024-11-10 17:34:00,253:INFO:SubProcess create_model() called ==================================
2024-11-10 17:34:00,253:INFO:Initializing create_model()
2024-11-10 17:34:00,253:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:34:00,253:INFO:Checking exceptions
2024-11-10 17:34:00,253:INFO:Importing libraries
2024-11-10 17:34:00,253:INFO:Copying training dataset
2024-11-10 17:34:00,257:INFO:Defining folds
2024-11-10 17:34:00,257:INFO:Declaring metric variables
2024-11-10 17:34:00,257:INFO:Importing untrained model
2024-11-10 17:34:00,258:INFO:Linear Discriminant Analysis Imported successfully
2024-11-10 17:34:00,258:INFO:Starting cross validation
2024-11-10 17:34:00,259:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:34:00,322:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,322:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,325:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 17:34:00,327:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 17:34:00,349:INFO:Calculating mean and std
2024-11-10 17:34:00,350:INFO:Creating metrics dataframe
2024-11-10 17:34:00,351:INFO:Uploading results into container
2024-11-10 17:34:00,351:INFO:Uploading model into container now
2024-11-10 17:34:00,352:INFO:_master_model_container: 11
2024-11-10 17:34:00,352:INFO:_display_container: 2
2024-11-10 17:34:00,352:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-10 17:34:00,352:INFO:create_model() successfully completed......................................
2024-11-10 17:34:00,405:INFO:SubProcess create_model() end ==================================
2024-11-10 17:34:00,405:INFO:Creating metrics dataframe
2024-11-10 17:34:00,408:INFO:Initializing Extra Trees Classifier
2024-11-10 17:34:00,408:INFO:Total runtime is 0.1295254707336426 minutes
2024-11-10 17:34:00,408:INFO:SubProcess create_model() called ==================================
2024-11-10 17:34:00,408:INFO:Initializing create_model()
2024-11-10 17:34:00,408:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:34:00,408:INFO:Checking exceptions
2024-11-10 17:34:00,409:INFO:Importing libraries
2024-11-10 17:34:00,409:INFO:Copying training dataset
2024-11-10 17:34:00,413:INFO:Defining folds
2024-11-10 17:34:00,413:INFO:Declaring metric variables
2024-11-10 17:34:00,413:INFO:Importing untrained model
2024-11-10 17:34:00,413:INFO:Extra Trees Classifier Imported successfully
2024-11-10 17:34:00,413:INFO:Starting cross validation
2024-11-10 17:34:00,414:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 17:34:00,822:INFO:Calculating mean and std
2024-11-10 17:34:00,823:INFO:Creating metrics dataframe
2024-11-10 17:34:00,824:INFO:Uploading results into container
2024-11-10 17:34:00,824:INFO:Uploading model into container now
2024-11-10 17:34:00,825:INFO:_master_model_container: 12
2024-11-10 17:34:00,825:INFO:_display_container: 2
2024-11-10 17:34:00,825:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-10 17:34:00,825:INFO:create_model() successfully completed......................................
2024-11-10 17:34:00,880:INFO:SubProcess create_model() end ==================================
2024-11-10 17:34:00,880:INFO:Creating metrics dataframe
2024-11-10 17:34:00,882:INFO:Initializing Light Gradient Boosting Machine
2024-11-10 17:34:00,882:INFO:Total runtime is 0.13742364645004274 minutes
2024-11-10 17:34:00,882:INFO:SubProcess create_model() called ==================================
2024-11-10 17:34:00,882:INFO:Initializing create_model()
2024-11-10 17:34:00,882:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7a69570abc10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x7a68c59c1450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 17:34:00,882:INFO:Checking exceptions
2024-11-10 17:34:00,882:INFO:Importing libraries
2024-11-10 17:34:00,882:INFO:Copying training dataset
2024-11-10 17:34:00,886:INFO:Defining folds
2024-11-10 17:34:00,886:INFO:Declaring metric variables
2024-11-10 17:34:00,886:INFO:Importing untrained model
2024-11-10 17:34:00,887:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-10 17:34:00,887:INFO:Starting cross validation
2024-11-10 17:34:00,887:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:44,645:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 18:42:45,396:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 18:42:45,396:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 18:42:45,396:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 18:42:45,396:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 18:42:45,780:INFO:PyCaret ClassificationExperiment
2024-11-10 18:42:45,780:INFO:Logging name: clf-default-name
2024-11-10 18:42:45,780:INFO:ML Usecase: MLUsecase.CLASSIFICATION
2024-11-10 18:42:45,780:INFO:version 3.3.2
2024-11-10 18:42:45,780:INFO:Initializing setup()
2024-11-10 18:42:45,780:INFO:self.USI: f0b5
2024-11-10 18:42:45,780:INFO:self._variable_keys: {'y', 'idx', 'is_multiclass', 'pipeline', 'n_jobs_param', 'X', 'gpu_param', 'log_plots_param', 'seed', 'gpu_n_jobs_param', 'fix_imbalance', 'y_test', 'data', 'html_param', 'X_train', 'memory', 'exp_id', 'target_param', '_ml_usecase', 'USI', 'exp_name_log', 'X_test', 'fold_generator', 'y_train', '_available_plots', 'fold_shuffle_param', 'logging_param', 'fold_groups_param'}
2024-11-10 18:42:45,780:INFO:Checking environment
2024-11-10 18:42:45,780:INFO:python_version: 3.10.12
2024-11-10 18:42:45,780:INFO:python_build: ('main', 'Sep 11 2024 15:47:36')
2024-11-10 18:42:45,780:INFO:machine: x86_64
2024-11-10 18:42:45,781:INFO:platform: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 18:42:45,781:INFO:Memory: svmem(total=16587202560, available=11156549632, percent=32.7, used=4493291520, free=3543740416, active=8077717504, inactive=3810652160, buffers=204910592, cached=8345260032, shared=589291520, slab=807997440)
2024-11-10 18:42:45,781:INFO:Physical Core: 6
2024-11-10 18:42:45,782:INFO:Logical Core: 12
2024-11-10 18:42:45,782:INFO:Checking libraries
2024-11-10 18:42:45,782:INFO:System:
2024-11-10 18:42:45,782:INFO:    python: 3.10.12 (main, Sep 11 2024, 15:47:36) [GCC 11.4.0]
2024-11-10 18:42:45,782:INFO:executable: /usr/bin/python3
2024-11-10 18:42:45,782:INFO:   machine: Linux-6.8.0-48-generic-x86_64-with-glibc2.35
2024-11-10 18:42:45,782:INFO:PyCaret required dependencies:
2024-11-10 18:42:45,795:INFO:                 pip: 22.0.2
2024-11-10 18:42:45,795:INFO:          setuptools: 59.6.0
2024-11-10 18:42:45,795:INFO:             pycaret: 3.3.2
2024-11-10 18:42:45,795:INFO:             IPython: 7.31.1
2024-11-10 18:42:45,795:INFO:          ipywidgets: 8.1.5
2024-11-10 18:42:45,795:INFO:                tqdm: 4.67.0
2024-11-10 18:42:45,795:INFO:               numpy: 1.21.5
2024-11-10 18:42:45,795:INFO:              pandas: 2.1.4
2024-11-10 18:42:45,795:INFO:              jinja2: 3.0.3
2024-11-10 18:42:45,795:INFO:               scipy: 1.8.0
2024-11-10 18:42:45,795:INFO:              joblib: 1.3.2
2024-11-10 18:42:45,795:INFO:             sklearn: 1.4.2
2024-11-10 18:42:45,795:INFO:                pyod: 2.0.2
2024-11-10 18:42:45,795:INFO:            imblearn: 0.12.4
2024-11-10 18:42:45,795:INFO:   category_encoders: 2.6.4
2024-11-10 18:42:45,795:INFO:            lightgbm: 4.5.0
2024-11-10 18:42:45,795:INFO:               numba: 0.60.0
2024-11-10 18:42:45,795:INFO:            requests: 2.25.1
2024-11-10 18:42:45,795:INFO:          matplotlib: 3.5.1
2024-11-10 18:42:45,795:INFO:          scikitplot: 0.3.7
2024-11-10 18:42:45,795:INFO:         yellowbrick: 1.5
2024-11-10 18:42:45,795:INFO:              plotly: 5.24.1
2024-11-10 18:42:45,795:INFO:    plotly-resampler: Not installed
2024-11-10 18:42:45,795:INFO:             kaleido: 0.2.1
2024-11-10 18:42:45,795:INFO:           schemdraw: 0.15
2024-11-10 18:42:45,795:INFO:         statsmodels: 0.14.4
2024-11-10 18:42:45,795:INFO:              sktime: 0.26.0
2024-11-10 18:42:45,795:INFO:               tbats: 1.1.3
2024-11-10 18:42:45,795:INFO:            pmdarima: 2.0.4
2024-11-10 18:42:45,795:INFO:              psutil: 6.1.0
2024-11-10 18:42:45,795:INFO:          markupsafe: 2.0.1
2024-11-10 18:42:45,795:INFO:             pickle5: Not installed
2024-11-10 18:42:45,795:INFO:         cloudpickle: 3.0.0
2024-11-10 18:42:45,796:INFO:         deprecation: 2.1.0
2024-11-10 18:42:45,796:INFO:              xxhash: 3.5.0
2024-11-10 18:42:45,796:INFO:           wurlitzer: 3.1.1
2024-11-10 18:42:45,796:INFO:PyCaret optional dependencies:
2024-11-10 18:42:46,137:INFO:                shap: Not installed
2024-11-10 18:42:46,137:INFO:           interpret: Not installed
2024-11-10 18:42:46,137:INFO:                umap: Not installed
2024-11-10 18:42:46,137:INFO:     ydata_profiling: Not installed
2024-11-10 18:42:46,137:INFO:  explainerdashboard: Not installed
2024-11-10 18:42:46,137:INFO:             autoviz: Not installed
2024-11-10 18:42:46,137:INFO:           fairlearn: Not installed
2024-11-10 18:42:46,137:INFO:          deepchecks: Not installed
2024-11-10 18:42:46,137:INFO:             xgboost: Not installed
2024-11-10 18:42:46,137:INFO:            catboost: Not installed
2024-11-10 18:42:46,137:INFO:              kmodes: Not installed
2024-11-10 18:42:46,137:INFO:             mlxtend: Not installed
2024-11-10 18:42:46,137:INFO:       statsforecast: Not installed
2024-11-10 18:42:46,137:INFO:        tune_sklearn: Not installed
2024-11-10 18:42:46,137:INFO:                 ray: Not installed
2024-11-10 18:42:46,138:INFO:            hyperopt: Not installed
2024-11-10 18:42:46,138:INFO:              optuna: Not installed
2024-11-10 18:42:46,138:INFO:               skopt: Not installed
2024-11-10 18:42:46,138:INFO:              mlflow: Not installed
2024-11-10 18:42:46,138:INFO:              gradio: Not installed
2024-11-10 18:42:46,138:INFO:             fastapi: 0.115.4
2024-11-10 18:42:46,138:INFO:             uvicorn: 0.15.0
2024-11-10 18:42:46,138:INFO:              m2cgen: Not installed
2024-11-10 18:42:46,138:INFO:           evidently: Not installed
2024-11-10 18:42:46,138:INFO:               fugue: Not installed
2024-11-10 18:42:46,138:INFO:           streamlit: Not installed
2024-11-10 18:42:46,138:INFO:             prophet: Not installed
2024-11-10 18:42:46,138:INFO:None
2024-11-10 18:42:46,138:INFO:Set up data.
2024-11-10 18:42:46,143:INFO:Set up folding strategy.
2024-11-10 18:42:46,143:INFO:Set up train/test split.
2024-11-10 18:42:46,147:INFO:Set up index.
2024-11-10 18:42:46,147:INFO:Assigning column types.
2024-11-10 18:42:46,151:INFO:Engine successfully changes for model 'lr' to 'sklearn'.
2024-11-10 18:42:46,181:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 18:42:46,183:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 18:42:46,205:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,205:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,236:INFO:Engine for model 'knn' has not been set explicitly, hence returning None.
2024-11-10 18:42:46,237:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 18:42:46,255:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,256:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,256:INFO:Engine successfully changes for model 'knn' to 'sklearn'.
2024-11-10 18:42:46,287:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 18:42:46,306:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,306:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,338:INFO:Engine for model 'rbfsvm' has not been set explicitly, hence returning None.
2024-11-10 18:42:46,356:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,357:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,357:INFO:Engine successfully changes for model 'rbfsvm' to 'sklearn'.
2024-11-10 18:42:46,407:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,407:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,457:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,457:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,458:INFO:Preparing preprocessing pipeline...
2024-11-10 18:42:46,459:INFO:Set up simple imputation.
2024-11-10 18:42:46,476:INFO:Finished creating preprocessing pipeline.
2024-11-10 18:42:46,479:INFO:Pipeline: Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False)
2024-11-10 18:42:46,479:INFO:Creating final display dataframe.
2024-11-10 18:42:46,535:INFO:Setup _display_container:                     Description             Value
0                    Session id               123
1                        Target        GradeClass
2                   Target type        Multiclass
3           Original data shape        (2392, 15)
4        Transformed data shape        (2392, 14)
5   Transformed train set shape        (1674, 14)
6    Transformed test set shape         (718, 14)
7               Ignore features                 1
8              Numeric features                13
9                    Preprocess              True
10              Imputation type            simple
11           Numeric imputation              mean
12       Categorical imputation              mode
13               Fold Generator   StratifiedKFold
14                  Fold Number                10
15                     CPU Jobs                -1
16                      Use GPU             False
17               Log Experiment             False
18              Experiment Name  clf-default-name
19                          USI              f0b5
2024-11-10 18:42:46,591:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,591:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,642:WARNING:
'xgboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install xgboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,642:WARNING:
'catboost' is a soft dependency and not included in the pycaret installation. Please run: `pip install catboost` to install.
Alternately, you can install this by running `pip install pycaret[models]`
2024-11-10 18:42:46,643:INFO:setup() successfully completed in 0.86s...............
2024-11-10 18:42:46,643:INFO:Initializing compare_models()
2024-11-10 18:42:46,643:INFO:compare_models(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, include=None, fold=None, round=4, cross_validation=True, sort=Accuracy, n_select=1, budget_time=None, turbo=True, errors=ignore, fit_kwargs=None, groups=None, experiment_custom_tags=None, probability_threshold=None, verbose=True, parallel=None, caller_params={'self': <pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, 'include': None, 'exclude': None, 'fold': None, 'round': 4, 'cross_validation': True, 'sort': 'Accuracy', 'n_select': 1, 'budget_time': None, 'turbo': True, 'errors': 'ignore', 'fit_kwargs': None, 'groups': None, 'experiment_custom_tags': None, 'probability_threshold': None, 'engine': None, 'verbose': True, 'parallel': None, '__class__': <class 'pycaret.classification.oop.ClassificationExperiment'>}, exclude=None)
2024-11-10 18:42:46,643:INFO:Checking exceptions
2024-11-10 18:42:46,646:INFO:Preparing display monitor
2024-11-10 18:42:46,648:INFO:Initializing Logistic Regression
2024-11-10 18:42:46,648:INFO:Total runtime is 1.0848045349121093e-06 minutes
2024-11-10 18:42:46,649:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:46,649:INFO:Initializing create_model()
2024-11-10 18:42:46,649:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=lr, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:46,649:INFO:Checking exceptions
2024-11-10 18:42:46,649:INFO:Importing libraries
2024-11-10 18:42:46,649:INFO:Copying training dataset
2024-11-10 18:42:46,653:INFO:Defining folds
2024-11-10 18:42:46,653:INFO:Declaring metric variables
2024-11-10 18:42:46,654:INFO:Importing untrained model
2024-11-10 18:42:46,654:INFO:Logistic Regression Imported successfully
2024-11-10 18:42:46,654:INFO:Starting cross validation
2024-11-10 18:42:46,655:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:46,931:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:46,932:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:46,935:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:46,935:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:46,946:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:46,948:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:46,963:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:46,966:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:46,983:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:46,983:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:47,012:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:47,013:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:47,015:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:47,015:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:47,026:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:47,026:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:47,057:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:47,058:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:47,059:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
2024-11-10 18:42:47,060:WARNING:  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"
2024-11-10 18:42:48,661:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:48,674:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:48,733:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:48,743:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,161:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:49,167:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,207:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:49,213:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,218:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:49,224:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,266:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:49,272:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,300:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:49,307:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,367:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:49,373:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,380:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:49,386:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,391:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):
STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.

Increase the number of iterations (max_iter) or scale the data as shown in:
    https://scikit-learn.org/stable/modules/preprocessing.html
Please also refer to the documentation for alternative solver options:
    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression
  n_iter_i = _check_optimize_result(

2024-11-10 18:42:49,398:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:49,411:INFO:Calculating mean and std
2024-11-10 18:42:49,412:INFO:Creating metrics dataframe
2024-11-10 18:42:49,414:INFO:Uploading results into container
2024-11-10 18:42:49,415:INFO:Uploading model into container now
2024-11-10 18:42:49,415:INFO:_master_model_container: 1
2024-11-10 18:42:49,415:INFO:_display_container: 2
2024-11-10 18:42:49,415:INFO:LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,
                   intercept_scaling=1, l1_ratio=None, max_iter=1000,
                   multi_class='auto', n_jobs=None, penalty='l2',
                   random_state=123, solver='lbfgs', tol=0.0001, verbose=0,
                   warm_start=False)
2024-11-10 18:42:49,415:INFO:create_model() successfully completed......................................
2024-11-10 18:42:49,483:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:49,483:INFO:Creating metrics dataframe
2024-11-10 18:42:49,485:INFO:Initializing K Neighbors Classifier
2024-11-10 18:42:49,485:INFO:Total runtime is 0.04727462927500407 minutes
2024-11-10 18:42:49,485:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:49,485:INFO:Initializing create_model()
2024-11-10 18:42:49,485:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=knn, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:49,485:INFO:Checking exceptions
2024-11-10 18:42:49,485:INFO:Importing libraries
2024-11-10 18:42:49,485:INFO:Copying training dataset
2024-11-10 18:42:49,489:INFO:Defining folds
2024-11-10 18:42:49,489:INFO:Declaring metric variables
2024-11-10 18:42:49,490:INFO:Importing untrained model
2024-11-10 18:42:49,490:INFO:K Neighbors Classifier Imported successfully
2024-11-10 18:42:49,490:INFO:Starting cross validation
2024-11-10 18:42:49,491:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:50,618:INFO:Calculating mean and std
2024-11-10 18:42:50,618:INFO:Creating metrics dataframe
2024-11-10 18:42:50,620:INFO:Uploading results into container
2024-11-10 18:42:50,620:INFO:Uploading model into container now
2024-11-10 18:42:50,621:INFO:_master_model_container: 2
2024-11-10 18:42:50,621:INFO:_display_container: 2
2024-11-10 18:42:50,621:INFO:KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',
                     metric_params=None, n_jobs=-1, n_neighbors=5, p=2,
                     weights='uniform')
2024-11-10 18:42:50,621:INFO:create_model() successfully completed......................................
2024-11-10 18:42:50,686:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:50,686:INFO:Creating metrics dataframe
2024-11-10 18:42:50,688:INFO:Initializing Naive Bayes
2024-11-10 18:42:50,688:INFO:Total runtime is 0.06732654968897502 minutes
2024-11-10 18:42:50,688:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:50,688:INFO:Initializing create_model()
2024-11-10 18:42:50,688:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=nb, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:50,688:INFO:Checking exceptions
2024-11-10 18:42:50,688:INFO:Importing libraries
2024-11-10 18:42:50,688:INFO:Copying training dataset
2024-11-10 18:42:50,692:INFO:Defining folds
2024-11-10 18:42:50,692:INFO:Declaring metric variables
2024-11-10 18:42:50,692:INFO:Importing untrained model
2024-11-10 18:42:50,693:INFO:Naive Bayes Imported successfully
2024-11-10 18:42:50,693:INFO:Starting cross validation
2024-11-10 18:42:50,693:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:50,729:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:50,733:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:50,737:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:50,738:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:50,743:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:50,744:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:50,747:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:50,750:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:50,761:INFO:Calculating mean and std
2024-11-10 18:42:50,762:INFO:Creating metrics dataframe
2024-11-10 18:42:50,763:INFO:Uploading results into container
2024-11-10 18:42:50,764:INFO:Uploading model into container now
2024-11-10 18:42:50,764:INFO:_master_model_container: 3
2024-11-10 18:42:50,764:INFO:_display_container: 2
2024-11-10 18:42:50,764:INFO:GaussianNB(priors=None, var_smoothing=1e-09)
2024-11-10 18:42:50,764:INFO:create_model() successfully completed......................................
2024-11-10 18:42:50,815:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:50,815:INFO:Creating metrics dataframe
2024-11-10 18:42:50,817:INFO:Initializing Decision Tree Classifier
2024-11-10 18:42:50,817:INFO:Total runtime is 0.06948334376017253 minutes
2024-11-10 18:42:50,818:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:50,818:INFO:Initializing create_model()
2024-11-10 18:42:50,818:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=dt, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:50,818:INFO:Checking exceptions
2024-11-10 18:42:50,818:INFO:Importing libraries
2024-11-10 18:42:50,818:INFO:Copying training dataset
2024-11-10 18:42:50,839:INFO:Defining folds
2024-11-10 18:42:50,840:INFO:Declaring metric variables
2024-11-10 18:42:50,841:INFO:Importing untrained model
2024-11-10 18:42:50,844:INFO:Decision Tree Classifier Imported successfully
2024-11-10 18:42:50,844:INFO:Starting cross validation
2024-11-10 18:42:50,844:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:50,914:INFO:Calculating mean and std
2024-11-10 18:42:50,915:INFO:Creating metrics dataframe
2024-11-10 18:42:50,920:INFO:Uploading results into container
2024-11-10 18:42:50,921:INFO:Uploading model into container now
2024-11-10 18:42:50,922:INFO:_master_model_container: 4
2024-11-10 18:42:50,922:INFO:_display_container: 2
2024-11-10 18:42:50,924:INFO:DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',
                       max_depth=None, max_features=None, max_leaf_nodes=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, random_state=123, splitter='best')
2024-11-10 18:42:50,924:INFO:create_model() successfully completed......................................
2024-11-10 18:42:50,989:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:50,989:INFO:Creating metrics dataframe
2024-11-10 18:42:50,991:INFO:Initializing SVM - Linear Kernel
2024-11-10 18:42:50,991:INFO:Total runtime is 0.07237837711970012 minutes
2024-11-10 18:42:50,991:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:50,991:INFO:Initializing create_model()
2024-11-10 18:42:50,991:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=svm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:50,992:INFO:Checking exceptions
2024-11-10 18:42:50,992:INFO:Importing libraries
2024-11-10 18:42:50,992:INFO:Copying training dataset
2024-11-10 18:42:50,996:INFO:Defining folds
2024-11-10 18:42:50,996:INFO:Declaring metric variables
2024-11-10 18:42:50,996:INFO:Importing untrained model
2024-11-10 18:42:50,996:INFO:SVM - Linear Kernel Imported successfully
2024-11-10 18:42:50,996:INFO:Starting cross validation
2024-11-10 18:42:50,997:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:51,053:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,061:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,068:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,068:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,073:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,074:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,077:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,081:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,081:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,082:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,083:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,085:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,085:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,086:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,087:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,088:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,090:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,094:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,099:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,119:INFO:Calculating mean and std
2024-11-10 18:42:51,120:INFO:Creating metrics dataframe
2024-11-10 18:42:51,121:INFO:Uploading results into container
2024-11-10 18:42:51,121:INFO:Uploading model into container now
2024-11-10 18:42:51,122:INFO:_master_model_container: 5
2024-11-10 18:42:51,122:INFO:_display_container: 2
2024-11-10 18:42:51,122:INFO:SGDClassifier(alpha=0.0001, average=False, class_weight=None,
              early_stopping=False, epsilon=0.1, eta0=0.001, fit_intercept=True,
              l1_ratio=0.15, learning_rate='optimal', loss='hinge',
              max_iter=1000, n_iter_no_change=5, n_jobs=-1, penalty='l2',
              power_t=0.5, random_state=123, shuffle=True, tol=0.001,
              validation_fraction=0.1, verbose=0, warm_start=False)
2024-11-10 18:42:51,122:INFO:create_model() successfully completed......................................
2024-11-10 18:42:51,176:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:51,176:INFO:Creating metrics dataframe
2024-11-10 18:42:51,178:INFO:Initializing Ridge Classifier
2024-11-10 18:42:51,178:INFO:Total runtime is 0.07549090782801311 minutes
2024-11-10 18:42:51,178:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:51,178:INFO:Initializing create_model()
2024-11-10 18:42:51,178:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=ridge, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:51,178:INFO:Checking exceptions
2024-11-10 18:42:51,178:INFO:Importing libraries
2024-11-10 18:42:51,179:INFO:Copying training dataset
2024-11-10 18:42:51,182:INFO:Defining folds
2024-11-10 18:42:51,182:INFO:Declaring metric variables
2024-11-10 18:42:51,183:INFO:Importing untrained model
2024-11-10 18:42:51,183:INFO:Ridge Classifier Imported successfully
2024-11-10 18:42:51,183:INFO:Starting cross validation
2024-11-10 18:42:51,184:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:51,208:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,213:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,216:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,217:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,217:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,218:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,221:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,221:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,222:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,223:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,223:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,223:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,223:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,224:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,225:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,227:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,228:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,229:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,230:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,231:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:51,241:INFO:Calculating mean and std
2024-11-10 18:42:51,242:INFO:Creating metrics dataframe
2024-11-10 18:42:51,243:INFO:Uploading results into container
2024-11-10 18:42:51,243:INFO:Uploading model into container now
2024-11-10 18:42:51,244:INFO:_master_model_container: 6
2024-11-10 18:42:51,244:INFO:_display_container: 2
2024-11-10 18:42:51,244:INFO:RidgeClassifier(alpha=1.0, class_weight=None, copy_X=True, fit_intercept=True,
                max_iter=None, positive=False, random_state=123, solver='auto',
                tol=0.0001)
2024-11-10 18:42:51,244:INFO:create_model() successfully completed......................................
2024-11-10 18:42:51,295:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:51,295:INFO:Creating metrics dataframe
2024-11-10 18:42:51,297:INFO:Initializing Random Forest Classifier
2024-11-10 18:42:51,297:INFO:Total runtime is 0.07748425801595053 minutes
2024-11-10 18:42:51,298:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:51,298:INFO:Initializing create_model()
2024-11-10 18:42:51,298:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=rf, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:51,298:INFO:Checking exceptions
2024-11-10 18:42:51,298:INFO:Importing libraries
2024-11-10 18:42:51,298:INFO:Copying training dataset
2024-11-10 18:42:51,302:INFO:Defining folds
2024-11-10 18:42:51,302:INFO:Declaring metric variables
2024-11-10 18:42:51,302:INFO:Importing untrained model
2024-11-10 18:42:51,302:INFO:Random Forest Classifier Imported successfully
2024-11-10 18:42:51,302:INFO:Starting cross validation
2024-11-10 18:42:51,303:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:51,728:INFO:Calculating mean and std
2024-11-10 18:42:51,728:INFO:Creating metrics dataframe
2024-11-10 18:42:51,730:INFO:Uploading results into container
2024-11-10 18:42:51,730:INFO:Uploading model into container now
2024-11-10 18:42:51,730:INFO:_master_model_container: 7
2024-11-10 18:42:51,730:INFO:_display_container: 2
2024-11-10 18:42:51,731:INFO:RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='gini', max_depth=None, max_features='sqrt',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_samples_leaf=1,
                       min_samples_split=2, min_weight_fraction_leaf=0.0,
                       monotonic_cst=None, n_estimators=100, n_jobs=-1,
                       oob_score=False, random_state=123, verbose=0,
                       warm_start=False)
2024-11-10 18:42:51,731:INFO:create_model() successfully completed......................................
2024-11-10 18:42:51,784:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:51,784:INFO:Creating metrics dataframe
2024-11-10 18:42:51,786:INFO:Initializing Quadratic Discriminant Analysis
2024-11-10 18:42:51,786:INFO:Total runtime is 0.08562563657760622 minutes
2024-11-10 18:42:51,786:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:51,786:INFO:Initializing create_model()
2024-11-10 18:42:51,786:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=qda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:51,786:INFO:Checking exceptions
2024-11-10 18:42:51,786:INFO:Importing libraries
2024-11-10 18:42:51,787:INFO:Copying training dataset
2024-11-10 18:42:51,791:INFO:Defining folds
2024-11-10 18:42:51,791:INFO:Declaring metric variables
2024-11-10 18:42:51,791:INFO:Importing untrained model
2024-11-10 18:42:51,791:INFO:Quadratic Discriminant Analysis Imported successfully
2024-11-10 18:42:51,791:INFO:Starting cross validation
2024-11-10 18:42:51,792:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:51,821:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,822:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,822:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,824:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,825:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,829:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,831:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,833:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,835:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,835:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:51,850:INFO:Calculating mean and std
2024-11-10 18:42:51,850:INFO:Creating metrics dataframe
2024-11-10 18:42:51,852:INFO:Uploading results into container
2024-11-10 18:42:51,852:INFO:Uploading model into container now
2024-11-10 18:42:51,852:INFO:_master_model_container: 8
2024-11-10 18:42:51,852:INFO:_display_container: 2
2024-11-10 18:42:51,852:INFO:QuadraticDiscriminantAnalysis(priors=None, reg_param=0.0,
                              store_covariance=False, tol=0.0001)
2024-11-10 18:42:51,852:INFO:create_model() successfully completed......................................
2024-11-10 18:42:51,903:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:51,903:INFO:Creating metrics dataframe
2024-11-10 18:42:51,905:INFO:Initializing Ada Boost Classifier
2024-11-10 18:42:51,905:INFO:Total runtime is 0.08761592706044517 minutes
2024-11-10 18:42:51,905:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:51,906:INFO:Initializing create_model()
2024-11-10 18:42:51,906:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=ada, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:51,906:INFO:Checking exceptions
2024-11-10 18:42:51,906:INFO:Importing libraries
2024-11-10 18:42:51,906:INFO:Copying training dataset
2024-11-10 18:42:51,910:INFO:Defining folds
2024-11-10 18:42:51,910:INFO:Declaring metric variables
2024-11-10 18:42:51,910:INFO:Importing untrained model
2024-11-10 18:42:51,910:INFO:Ada Boost Classifier Imported successfully
2024-11-10 18:42:51,910:INFO:Starting cross validation
2024-11-10 18:42:51,911:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:51,929:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,929:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,930:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,931:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,934:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,936:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,936:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,940:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,941:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:51,942:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/ensemble/_weight_boosting.py:519: FutureWarning: The SAMME.R algorithm (the default) is deprecated and will be removed in 1.6. Use the SAMME algorithm to circumvent this warning.
  warnings.warn(

2024-11-10 18:42:52,046:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,050:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,097:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,106:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,109:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,111:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,112:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,115:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,115:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,120:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:52,140:INFO:Calculating mean and std
2024-11-10 18:42:52,141:INFO:Creating metrics dataframe
2024-11-10 18:42:52,142:INFO:Uploading results into container
2024-11-10 18:42:52,142:INFO:Uploading model into container now
2024-11-10 18:42:52,143:INFO:_master_model_container: 9
2024-11-10 18:42:52,143:INFO:_display_container: 2
2024-11-10 18:42:52,143:INFO:AdaBoostClassifier(algorithm='SAMME.R', estimator=None, learning_rate=1.0,
                   n_estimators=50, random_state=123)
2024-11-10 18:42:52,143:INFO:create_model() successfully completed......................................
2024-11-10 18:42:52,194:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:52,194:INFO:Creating metrics dataframe
2024-11-10 18:42:52,196:INFO:Initializing Gradient Boosting Classifier
2024-11-10 18:42:52,196:INFO:Total runtime is 0.09246635437011722 minutes
2024-11-10 18:42:52,197:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:52,197:INFO:Initializing create_model()
2024-11-10 18:42:52,197:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=gbc, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:52,197:INFO:Checking exceptions
2024-11-10 18:42:52,197:INFO:Importing libraries
2024-11-10 18:42:52,197:INFO:Copying training dataset
2024-11-10 18:42:52,201:INFO:Defining folds
2024-11-10 18:42:52,201:INFO:Declaring metric variables
2024-11-10 18:42:52,201:INFO:Importing untrained model
2024-11-10 18:42:52,201:INFO:Gradient Boosting Classifier Imported successfully
2024-11-10 18:42:52,202:INFO:Starting cross validation
2024-11-10 18:42:52,202:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:53,347:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,368:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,672:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,692:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,719:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,723:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,729:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,731:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,732:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,735:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,754:INFO:Calculating mean and std
2024-11-10 18:42:53,754:INFO:Creating metrics dataframe
2024-11-10 18:42:53,756:INFO:Uploading results into container
2024-11-10 18:42:53,756:INFO:Uploading model into container now
2024-11-10 18:42:53,756:INFO:_master_model_container: 10
2024-11-10 18:42:53,756:INFO:_display_container: 2
2024-11-10 18:42:53,757:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-10 18:42:53,757:INFO:create_model() successfully completed......................................
2024-11-10 18:42:53,824:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:53,824:INFO:Creating metrics dataframe
2024-11-10 18:42:53,826:INFO:Initializing Linear Discriminant Analysis
2024-11-10 18:42:53,826:INFO:Total runtime is 0.11963021755218509 minutes
2024-11-10 18:42:53,826:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:53,827:INFO:Initializing create_model()
2024-11-10 18:42:53,827:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=lda, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:53,827:INFO:Checking exceptions
2024-11-10 18:42:53,827:INFO:Importing libraries
2024-11-10 18:42:53,827:INFO:Copying training dataset
2024-11-10 18:42:53,831:INFO:Defining folds
2024-11-10 18:42:53,831:INFO:Declaring metric variables
2024-11-10 18:42:53,831:INFO:Importing untrained model
2024-11-10 18:42:53,831:INFO:Linear Discriminant Analysis Imported successfully
2024-11-10 18:42:53,831:INFO:Starting cross validation
2024-11-10 18:42:53,832:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:53,863:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,864:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,864:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,865:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,867:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,868:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,870:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,870:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,872:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,876:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py:204: FitFailedWarning: Metric 'make_scorer(roc_auc_score, response_method=('decision_function', 'predict_proba'), average=weighted, multi_class=ovr)' failed and error score 0.0 has been returned instead. If this is a custom metric, this usually means that the error is in the metric code. Full exception below:
Traceback (most recent call last):
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 196, in _score
    return super()._score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_scorer.py", line 350, in _score
    return self._sign * self._score_func(y_true, y_pred, **scoring_kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/metrics.py", line 144, in __call__
    return self.score_func(y_true, y_pred, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/utils/_param_validation.py", line 213, in wrapper
    return func(*args, **kwargs)
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 634, in roc_auc_score
    return _multiclass_roc_auc_score(
  File "/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_ranking.py", line 707, in _multiclass_roc_auc_score
    raise ValueError(
ValueError: Target scores need to be probabilities for multiclass roc_auc, i.e. they should sum up to 1.0 over classes

  warnings.warn(

2024-11-10 18:42:53,880:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 18:42:53,889:INFO:Calculating mean and std
2024-11-10 18:42:53,890:INFO:Creating metrics dataframe
2024-11-10 18:42:53,891:INFO:Uploading results into container
2024-11-10 18:42:53,892:INFO:Uploading model into container now
2024-11-10 18:42:53,892:INFO:_master_model_container: 11
2024-11-10 18:42:53,892:INFO:_display_container: 2
2024-11-10 18:42:53,892:INFO:LinearDiscriminantAnalysis(covariance_estimator=None, n_components=None,
                           priors=None, shrinkage=None, solver='svd',
                           store_covariance=False, tol=0.0001)
2024-11-10 18:42:53,892:INFO:create_model() successfully completed......................................
2024-11-10 18:42:53,944:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:53,945:INFO:Creating metrics dataframe
2024-11-10 18:42:53,947:INFO:Initializing Extra Trees Classifier
2024-11-10 18:42:53,947:INFO:Total runtime is 0.12163868347803754 minutes
2024-11-10 18:42:53,947:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:53,947:INFO:Initializing create_model()
2024-11-10 18:42:53,947:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=et, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:53,947:INFO:Checking exceptions
2024-11-10 18:42:53,947:INFO:Importing libraries
2024-11-10 18:42:53,947:INFO:Copying training dataset
2024-11-10 18:42:53,951:INFO:Defining folds
2024-11-10 18:42:53,951:INFO:Declaring metric variables
2024-11-10 18:42:53,951:INFO:Importing untrained model
2024-11-10 18:42:53,952:INFO:Extra Trees Classifier Imported successfully
2024-11-10 18:42:53,952:INFO:Starting cross validation
2024-11-10 18:42:53,952:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 18:42:54,346:INFO:Calculating mean and std
2024-11-10 18:42:54,347:INFO:Creating metrics dataframe
2024-11-10 18:42:54,348:INFO:Uploading results into container
2024-11-10 18:42:54,349:INFO:Uploading model into container now
2024-11-10 18:42:54,349:INFO:_master_model_container: 12
2024-11-10 18:42:54,349:INFO:_display_container: 2
2024-11-10 18:42:54,349:INFO:ExtraTreesClassifier(bootstrap=False, ccp_alpha=0.0, class_weight=None,
                     criterion='gini', max_depth=None, max_features='sqrt',
                     max_leaf_nodes=None, max_samples=None,
                     min_impurity_decrease=0.0, min_samples_leaf=1,
                     min_samples_split=2, min_weight_fraction_leaf=0.0,
                     monotonic_cst=None, n_estimators=100, n_jobs=-1,
                     oob_score=False, random_state=123, verbose=0,
                     warm_start=False)
2024-11-10 18:42:54,349:INFO:create_model() successfully completed......................................
2024-11-10 18:42:54,403:INFO:SubProcess create_model() end ==================================
2024-11-10 18:42:54,403:INFO:Creating metrics dataframe
2024-11-10 18:42:54,406:INFO:Initializing Light Gradient Boosting Machine
2024-11-10 18:42:54,406:INFO:Total runtime is 0.12928777138392133 minutes
2024-11-10 18:42:54,406:INFO:SubProcess create_model() called ==================================
2024-11-10 18:42:54,406:INFO:Initializing create_model()
2024-11-10 18:42:54,406:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=lightgbm, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 18:42:54,406:INFO:Checking exceptions
2024-11-10 18:42:54,406:INFO:Importing libraries
2024-11-10 18:42:54,406:INFO:Copying training dataset
2024-11-10 18:42:54,411:INFO:Defining folds
2024-11-10 18:42:54,411:INFO:Declaring metric variables
2024-11-10 18:42:54,411:INFO:Importing untrained model
2024-11-10 18:42:54,411:INFO:Light Gradient Boosting Machine Imported successfully
2024-11-10 18:42:54,411:INFO:Starting cross validation
2024-11-10 18:42:54,412:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 20:11:55,134:INFO:Calculating mean and std
2024-11-10 20:11:55,135:INFO:Creating metrics dataframe
2024-11-10 20:11:55,137:INFO:Uploading results into container
2024-11-10 20:11:55,137:INFO:Uploading model into container now
2024-11-10 20:11:55,137:INFO:_master_model_container: 13
2024-11-10 20:11:55,137:INFO:_display_container: 2
2024-11-10 20:11:55,138:INFO:LGBMClassifier(boosting_type='gbdt', class_weight=None, colsample_bytree=1.0,
               importance_type='split', learning_rate=0.1, max_depth=-1,
               min_child_samples=20, min_child_weight=0.001, min_split_gain=0.0,
               n_estimators=100, n_jobs=-1, num_leaves=31, objective=None,
               random_state=123, reg_alpha=0.0, reg_lambda=0.0, subsample=1.0,
               subsample_for_bin=200000, subsample_freq=0)
2024-11-10 20:11:55,138:INFO:create_model() successfully completed......................................
2024-11-10 20:11:55,198:INFO:SubProcess create_model() end ==================================
2024-11-10 20:11:55,198:INFO:Creating metrics dataframe
2024-11-10 20:11:55,200:INFO:Initializing Dummy Classifier
2024-11-10 20:11:55,200:INFO:Total runtime is 89.14253461360931 minutes
2024-11-10 20:11:55,201:INFO:SubProcess create_model() called ==================================
2024-11-10 20:11:55,201:INFO:Initializing create_model()
2024-11-10 20:11:55,201:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=dummy, fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=True, predict=True, fit_kwargs={}, groups=None, refit=False, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=<pycaret.internal.display.display.CommonDisplay object at 0x795b373f5450>, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 20:11:55,201:INFO:Checking exceptions
2024-11-10 20:11:55,201:INFO:Importing libraries
2024-11-10 20:11:55,201:INFO:Copying training dataset
2024-11-10 20:11:55,205:INFO:Defining folds
2024-11-10 20:11:55,205:INFO:Declaring metric variables
2024-11-10 20:11:55,205:INFO:Importing untrained model
2024-11-10 20:11:55,205:INFO:Dummy Classifier Imported successfully
2024-11-10 20:11:55,206:INFO:Starting cross validation
2024-11-10 20:11:55,206:INFO:Cross validating with StratifiedKFold(n_splits=10, random_state=None, shuffle=False), n_jobs=-1
2024-11-10 20:11:55,242:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,255:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,256:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,258:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,266:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,270:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,270:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,270:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,273:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,274:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1509: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.
  _warn_prf(average, modifier, f"{metric.capitalize()} is", len(result))

2024-11-10 20:11:55,288:INFO:Calculating mean and std
2024-11-10 20:11:55,288:INFO:Creating metrics dataframe
2024-11-10 20:11:55,290:INFO:Uploading results into container
2024-11-10 20:11:55,291:INFO:Uploading model into container now
2024-11-10 20:11:55,291:INFO:_master_model_container: 14
2024-11-10 20:11:55,291:INFO:_display_container: 2
2024-11-10 20:11:55,291:INFO:DummyClassifier(constant=None, random_state=123, strategy='prior')
2024-11-10 20:11:55,291:INFO:create_model() successfully completed......................................
2024-11-10 20:11:55,358:INFO:SubProcess create_model() end ==================================
2024-11-10 20:11:55,359:INFO:Creating metrics dataframe
2024-11-10 20:11:55,361:WARNING:/home/houcem-bm/.local/lib/python3.10/site-packages/pycaret/internal/pycaret_experiment/supervised_experiment.py:323: FutureWarning: Styler.applymap has been deprecated. Use Styler.map instead.
  master_display_.apply(

2024-11-10 20:11:55,363:INFO:Initializing create_model()
2024-11-10 20:11:55,363:INFO:create_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x795bc8107c10>, estimator=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), fold=StratifiedKFold(n_splits=10, random_state=None, shuffle=False), round=4, cross_validation=False, predict=False, fit_kwargs={}, groups=None, refit=True, probability_threshold=None, experiment_custom_tags=None, verbose=False, system=False, add_to_model_list=True, metrics=None, display=None, model_only=True, return_train_score=False, error_score=0.0, kwargs={})
2024-11-10 20:11:55,363:INFO:Checking exceptions
2024-11-10 20:11:55,363:INFO:Importing libraries
2024-11-10 20:11:55,363:INFO:Copying training dataset
2024-11-10 20:11:55,368:INFO:Defining folds
2024-11-10 20:11:55,368:INFO:Declaring metric variables
2024-11-10 20:11:55,368:INFO:Importing untrained model
2024-11-10 20:11:55,368:INFO:Declaring custom model
2024-11-10 20:11:55,369:INFO:Gradient Boosting Classifier Imported successfully
2024-11-10 20:11:55,369:INFO:Cross validation set to False
2024-11-10 20:11:55,369:INFO:Fitting Model
2024-11-10 20:11:56,484:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-10 20:11:56,485:INFO:create_model() successfully completed......................................
2024-11-10 20:11:56,543:INFO:_master_model_container: 14
2024-11-10 20:11:56,544:INFO:_display_container: 2
2024-11-10 20:11:56,544:INFO:GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False)
2024-11-10 20:11:56,544:INFO:compare_models() successfully completed......................................
2024-11-10 20:11:56,547:INFO:Initializing save_model()
2024-11-10 20:11:56,547:INFO:save_model(model=GradientBoostingClassifier(ccp_alpha=0.0, criterion='friedman_mse', init=None,
                           learning_rate=0.1, loss='log_loss', max_depth=3,
                           max_features=None, max_leaf_nodes=None,
                           min_impurity_decrease=0.0, min_samples_leaf=1,
                           min_samples_split=2, min_weight_fraction_leaf=0.0,
                           n_estimators=100, n_iter_no_change=None,
                           random_state=123, subsample=1.0, tol=0.0001,
                           validation_fraction=0.1, verbose=0,
                           warm_start=False), model_name=bestModel, prep_pipe_=Pipeline(memory=FastMemory(location=/tmp/joblib),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='mean'))),
                ('categorical_imputer',
                 TransformerWrapper(exclude=None, include=[],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empty_features=False,
                                                              missing_values=nan,
                                                              strategy='most_frequent')))],
         verbose=False), verbose=True, use_case=MLUsecase.CLASSIFICATION, kwargs={})
2024-11-10 20:11:56,547:INFO:Adding model into prep_pipe
2024-11-10 20:11:56,562:INFO:bestModel.pkl saved in current working directory
2024-11-10 20:11:56,566:INFO:Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(exclude=None,
                                    include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer(add_indicator=False,
                                                              copy=True,
                                                              fill_value=None,
                                                              keep_empt...
                                            criterion='friedman_mse', init=None,
                                            learning_rate=0.1, loss='log_loss',
                                            max_depth=3, max_features=None,
                                            max_leaf_nodes=None,
                                            min_impurity_decrease=0.0,
                                            min_samples_leaf=1,
                                            min_samples_split=2,
                                            min_weight_fraction_leaf=0.0,
                                            n_estimators=100,
                                            n_iter_no_change=None,
                                            random_state=123, subsample=1.0,
                                            tol=0.0001, validation_fraction=0.1,
                                            verbose=0, warm_start=False))],
         verbose=False)
2024-11-10 20:11:56,566:INFO:save_model() successfully completed......................................
2024-11-10 20:24:19,886:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:24:20,649:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:20,649:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:20,649:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:20,649:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:21,115:INFO:Initializing load_model()
2024-11-10 20:24:21,115:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2024-11-10 20:24:42,771:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:24:43,601:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:43,601:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:43,601:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:43,601:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:44,063:INFO:Initializing load_model()
2024-11-10 20:24:44,063:INFO:load_model(model_name=be, platform=None, authentication=None, verbose=True)
2024-11-10 20:24:45,780:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:24:46,563:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:46,563:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:46,563:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:46,563:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:24:46,974:INFO:Initializing load_model()
2024-11-10 20:24:46,974:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:25:51,308:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:25:52,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:25:52,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:25:52,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:25:52,149:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:25:52,580:INFO:Initializing load_model()
2024-11-10 20:25:52,580:INFO:load_model(model_name=student_performance_api, platform=None, authentication=None, verbose=True)
2024-11-10 20:25:59,422:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:26:00,248:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:00,248:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:00,248:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:00,248:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:01,295:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:26:02,105:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:02,105:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:02,105:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:02,105:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:03,340:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:26:04,123:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:04,123:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:04,123:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:04,123:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:26:04,552:INFO:Initializing load_model()
2024-11-10 20:26:04,552:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:27:08,409:INFO:Initializing predict_model()
2024-11-10 20:27:08,409:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x75f6629ab3d0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x75f66283af80>)
2024-11-10 20:27:08,409:INFO:Checking exceptions
2024-11-10 20:27:08,409:INFO:Preloading libraries
2024-11-10 20:27:08,409:INFO:Set up data.
2024-11-10 20:27:08,413:INFO:Set up index.
2024-11-10 20:27:50,171:INFO:Initializing predict_model()
2024-11-10 20:27:50,171:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x75f6629e6a70>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x75f66283b010>)
2024-11-10 20:27:50,171:INFO:Checking exceptions
2024-11-10 20:27:50,171:INFO:Preloading libraries
2024-11-10 20:27:50,172:INFO:Set up data.
2024-11-10 20:27:50,175:INFO:Set up index.
2024-11-10 20:27:51,460:INFO:Initializing predict_model()
2024-11-10 20:27:51,460:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x75f6628004f0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x75f661cee5f0>)
2024-11-10 20:27:51,460:INFO:Checking exceptions
2024-11-10 20:27:51,460:INFO:Preloading libraries
2024-11-10 20:27:51,460:INFO:Set up data.
2024-11-10 20:27:51,463:INFO:Set up index.
2024-11-10 20:30:02,154:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:30:02,977:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:02,978:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:02,978:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:02,978:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:03,450:INFO:Initializing load_model()
2024-11-10 20:30:03,450:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:30:24,081:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:30:25,903:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:30:26,757:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:26,757:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:26,757:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:26,758:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:27,191:INFO:Initializing load_model()
2024-11-10 20:30:27,191:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:30:30,387:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:30:31,969:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:30:32,982:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:32,982:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:32,982:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:32,982:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:33,486:INFO:Initializing load_model()
2024-11-10 20:30:33,486:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:30:35,392:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:30:36,343:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:36,344:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:36,344:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:36,344:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:36,807:INFO:Initializing load_model()
2024-11-10 20:30:36,807:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:30:38,482:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:30:39,352:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:39,353:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:39,353:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:39,353:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:40,483:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:30:41,344:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:41,344:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:41,344:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:41,344:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:30:41,824:INFO:Initializing load_model()
2024-11-10 20:30:41,824:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:32:13,672:INFO:Initializing predict_model()
2024-11-10 20:32:13,672:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x793f1239e920>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x793f1222ef80>)
2024-11-10 20:32:13,672:INFO:Checking exceptions
2024-11-10 20:32:13,672:INFO:Preloading libraries
2024-11-10 20:32:13,673:INFO:Set up data.
2024-11-10 20:32:13,676:INFO:Set up index.
2024-11-10 20:32:35,472:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:32:47,662:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:32:48,465:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:32:48,465:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:32:48,465:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:32:48,465:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:32:48,946:INFO:Initializing load_model()
2024-11-10 20:32:48,946:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:32:51,046:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:32:51,960:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:32:51,960:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:32:51,960:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:32:51,960:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:32:52,593:INFO:Initializing load_model()
2024-11-10 20:32:52,593:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:33:41,783:INFO:Initializing predict_model()
2024-11-10 20:33:41,783:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7d87da40b790>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7d87da45b1c0>)
2024-11-10 20:33:41,783:INFO:Checking exceptions
2024-11-10 20:33:41,784:INFO:Preloading libraries
2024-11-10 20:33:41,784:INFO:Set up data.
2024-11-10 20:33:41,787:INFO:Set up index.
2024-11-10 20:35:43,765:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:35:44,566:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:44,578:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:44,578:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:44,578:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:45,507:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:35:46,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:46,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:46,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:46,315:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:46,792:INFO:Initializing load_model()
2024-11-10 20:35:46,792:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:35:48,507:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:35:49,324:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:49,324:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:49,325:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:49,325:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:49,769:INFO:Initializing load_model()
2024-11-10 20:35:49,769:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:35:51,938:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:35:52,784:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:52,784:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:52,784:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:52,785:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:35:53,221:INFO:Initializing load_model()
2024-11-10 20:35:53,221:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:35:58,173:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:35:59,653:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:36:00,477:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:00,477:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:00,478:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:00,478:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:00,895:INFO:Initializing load_model()
2024-11-10 20:36:00,895:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:36:02,383:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:36:03,319:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:03,319:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:03,319:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:03,319:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:04,366:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:36:05,910:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:36:06,720:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:06,721:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:06,721:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:06,721:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:07,142:INFO:Initializing load_model()
2024-11-10 20:36:07,142:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:36:12,084:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:36:13,609:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:36:14,417:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:14,417:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:14,417:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:14,417:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:36:14,853:INFO:Initializing load_model()
2024-11-10 20:36:14,853:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:36:23,780:INFO:Initializing predict_model()
2024-11-10 20:36:23,780:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x77014a9fc9a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x77014a9017e0>)
2024-11-10 20:36:23,781:INFO:Checking exceptions
2024-11-10 20:36:23,781:INFO:Preloading libraries
2024-11-10 20:36:23,781:INFO:Set up data.
2024-11-10 20:36:23,785:INFO:Set up index.
2024-11-10 20:37:53,670:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:37:54,458:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:37:54,459:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:37:54,459:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:37:54,459:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:37:54,963:INFO:Initializing load_model()
2024-11-10 20:37:54,963:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:38:02,407:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:38:04,323:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:38:05,140:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:38:05,140:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:38:05,140:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:38:05,140:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:38:05,599:INFO:Initializing load_model()
2024-11-10 20:38:05,599:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:38:10,295:INFO:Initializing predict_model()
2024-11-10 20:38:10,295:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x75a183f149a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x75a1880197e0>)
2024-11-10 20:38:10,295:INFO:Checking exceptions
2024-11-10 20:38:10,295:INFO:Preloading libraries
2024-11-10 20:38:10,295:INFO:Set up data.
2024-11-10 20:38:10,299:INFO:Set up index.
2024-11-10 20:45:56,138:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:45:56,929:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:45:56,930:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:45:56,930:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:45:56,930:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:45:57,408:INFO:Initializing load_model()
2024-11-10 20:45:57,408:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:46:01,656:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:46:02,474:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:02,474:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:02,474:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:02,474:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:02,914:INFO:Initializing load_model()
2024-11-10 20:46:02,914:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:46:05,090:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:46:05,959:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:05,960:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:05,960:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:05,960:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:06,394:INFO:Initializing load_model()
2024-11-10 20:46:06,394:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:46:08,403:INFO:Initializing predict_model()
2024-11-10 20:46:08,403:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x76ca1a6fc9a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x76ca1a6057e0>)
2024-11-10 20:46:08,403:INFO:Checking exceptions
2024-11-10 20:46:08,403:INFO:Preloading libraries
2024-11-10 20:46:08,403:INFO:Set up data.
2024-11-10 20:46:08,407:INFO:Set up index.
2024-11-10 20:46:17,546:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:46:18,347:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:18,347:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:18,347:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:18,347:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:46:18,778:INFO:Initializing load_model()
2024-11-10 20:46:18,778:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:51:42,368:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:51:43,190:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:51:43,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:51:43,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:51:43,191:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:51:43,649:INFO:Initializing load_model()
2024-11-10 20:51:43,649:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:51:44,423:INFO:Initializing predict_model()
2024-11-10 20:51:44,423:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x77c4d0d289a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x77c4d0e2d7e0>)
2024-11-10 20:51:44,423:INFO:Checking exceptions
2024-11-10 20:51:44,423:INFO:Preloading libraries
2024-11-10 20:51:44,424:INFO:Set up data.
2024-11-10 20:51:44,427:INFO:Set up index.
2024-11-10 20:52:20,174:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:52:21,142:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:52:21,143:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:52:21,143:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:52:21,143:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:52:21,607:INFO:Initializing load_model()
2024-11-10 20:52:21,607:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:52:21,653:INFO:Initializing predict_model()
2024-11-10 20:52:21,653:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x752d09ebbd90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x752d09fd97e0>)
2024-11-10 20:52:21,653:INFO:Checking exceptions
2024-11-10 20:52:21,653:INFO:Preloading libraries
2024-11-10 20:52:21,653:INFO:Set up data.
2024-11-10 20:52:21,658:INFO:Set up index.
2024-11-10 20:55:15,246:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:55:16,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:55:16,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:55:16,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:55:16,104:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:55:16,636:INFO:Initializing load_model()
2024-11-10 20:55:16,636:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:55:17,987:INFO:Initializing predict_model()
2024-11-10 20:55:17,987:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7804bc5d89a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7804bc6dd7e0>)
2024-11-10 20:55:17,987:INFO:Checking exceptions
2024-11-10 20:55:17,987:INFO:Preloading libraries
2024-11-10 20:55:17,988:INFO:Set up data.
2024-11-10 20:55:17,993:INFO:Set up index.
2024-11-10 20:55:50,849:INFO:Initializing predict_model()
2024-11-10 20:55:50,849:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7804bc5f9390>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7804bc440700>)
2024-11-10 20:55:50,849:INFO:Checking exceptions
2024-11-10 20:55:50,849:INFO:Preloading libraries
2024-11-10 20:55:50,849:INFO:Set up data.
2024-11-10 20:55:50,852:INFO:Set up index.
2024-11-10 20:56:21,886:INFO:Initializing predict_model()
2024-11-10 20:56:21,886:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7804bc5db5b0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7804bc4412d0>)
2024-11-10 20:56:21,886:INFO:Checking exceptions
2024-11-10 20:56:21,886:INFO:Preloading libraries
2024-11-10 20:56:21,887:INFO:Set up data.
2024-11-10 20:56:21,890:INFO:Set up index.
2024-11-10 20:56:27,558:INFO:Initializing predict_model()
2024-11-10 20:56:27,558:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7804bc5d8850>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7804bc4412d0>)
2024-11-10 20:56:27,558:INFO:Checking exceptions
2024-11-10 20:56:27,558:INFO:Preloading libraries
2024-11-10 20:56:27,558:INFO:Set up data.
2024-11-10 20:56:27,561:INFO:Set up index.
2024-11-10 20:56:47,566:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:56:48,378:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:56:48,378:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:56:48,378:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:56:48,378:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:56:48,824:INFO:Initializing load_model()
2024-11-10 20:56:48,824:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:56:49,013:INFO:Initializing predict_model()
2024-11-10 20:56:49,013:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x728c121089a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x728c122117e0>)
2024-11-10 20:56:49,013:INFO:Checking exceptions
2024-11-10 20:56:49,013:INFO:Preloading libraries
2024-11-10 20:56:49,013:INFO:Set up data.
2024-11-10 20:56:49,018:INFO:Set up index.
2024-11-10 20:56:55,259:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 20:56:56,089:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:56:56,089:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:56:56,089:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:56:56,089:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 20:56:56,536:INFO:Initializing load_model()
2024-11-10 20:56:56,536:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 20:56:56,582:INFO:Initializing predict_model()
2024-11-10 20:56:56,582:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7ee5789bfd90>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7ee578add7e0>)
2024-11-10 20:56:56,582:INFO:Checking exceptions
2024-11-10 20:56:56,582:INFO:Preloading libraries
2024-11-10 20:56:56,583:INFO:Set up data.
2024-11-10 20:56:56,587:INFO:Set up index.
2024-11-10 23:21:22,611:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:21:28,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:21:28,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:21:28,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:21:28,958:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:21:33,917:INFO:Initializing load_model()
2024-11-10 23:21:33,917:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 23:22:21,321:INFO:Initializing predict_model()
2024-11-10 23:22:21,321:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x775a307b60b0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x775a30642ef0>)
2024-11-10 23:22:21,321:INFO:Checking exceptions
2024-11-10 23:22:21,321:INFO:Preloading libraries
2024-11-10 23:22:21,333:INFO:Set up data.
2024-11-10 23:22:21,341:INFO:Set up index.
2024-11-10 23:22:57,780:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:22:58,643:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:22:58,643:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:22:58,643:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:22:58,643:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:22:59,169:INFO:Initializing load_model()
2024-11-10 23:22:59,169:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 23:22:59,214:INFO:Initializing predict_model()
2024-11-10 23:22:59,214:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x760e056e3df0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x760e056017e0>)
2024-11-10 23:22:59,214:INFO:Checking exceptions
2024-11-10 23:22:59,214:INFO:Preloading libraries
2024-11-10 23:22:59,214:INFO:Set up data.
2024-11-10 23:22:59,219:INFO:Set up index.
2024-11-10 23:23:31,511:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:23:32,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:23:32,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:23:32,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:23:32,421:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:23:32,922:INFO:Initializing load_model()
2024-11-10 23:23:32,922:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 23:23:48,072:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:23:48,919:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:23:48,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:23:48,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:23:48,920:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:23:49,463:INFO:Initializing load_model()
2024-11-10 23:23:49,463:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 23:23:50,991:INFO:Initializing predict_model()
2024-11-10 23:23:50,991:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef17eca00>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef18f1900>)
2024-11-10 23:23:50,991:INFO:Checking exceptions
2024-11-10 23:23:50,991:INFO:Preloading libraries
2024-11-10 23:23:50,991:INFO:Set up data.
2024-11-10 23:23:50,995:INFO:Set up index.
2024-11-10 23:32:16,136:INFO:Initializing predict_model()
2024-11-10 23:32:16,136:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160af80>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163ff40>)
2024-11-10 23:32:16,136:INFO:Checking exceptions
2024-11-10 23:32:16,136:INFO:Preloading libraries
2024-11-10 23:32:16,136:INFO:Set up data.
2024-11-10 23:32:16,139:INFO:Set up index.
2024-11-10 23:32:17,754:INFO:Initializing predict_model()
2024-11-10 23:32:17,754:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160bcd0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:32:17,754:INFO:Checking exceptions
2024-11-10 23:32:17,754:INFO:Preloading libraries
2024-11-10 23:32:17,755:INFO:Set up data.
2024-11-10 23:32:17,764:INFO:Set up index.
2024-11-10 23:32:23,767:INFO:Initializing predict_model()
2024-11-10 23:32:23,767:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160a6e0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163d900>)
2024-11-10 23:32:23,767:INFO:Checking exceptions
2024-11-10 23:32:23,767:INFO:Preloading libraries
2024-11-10 23:32:23,767:INFO:Set up data.
2024-11-10 23:32:23,770:INFO:Set up index.
2024-11-10 23:32:24,414:INFO:Initializing predict_model()
2024-11-10 23:32:24,414:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160afb0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:32:24,414:INFO:Checking exceptions
2024-11-10 23:32:24,414:INFO:Preloading libraries
2024-11-10 23:32:24,415:INFO:Set up data.
2024-11-10 23:32:24,417:INFO:Set up index.
2024-11-10 23:32:53,645:INFO:Initializing predict_model()
2024-11-10 23:32:53,645:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1608460>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163f910>)
2024-11-10 23:32:53,645:INFO:Checking exceptions
2024-11-10 23:32:53,645:INFO:Preloading libraries
2024-11-10 23:32:53,646:INFO:Set up data.
2024-11-10 23:32:53,648:INFO:Set up index.
2024-11-10 23:37:26,643:INFO:Initializing predict_model()
2024-11-10 23:37:26,643:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160aaa0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163f910>)
2024-11-10 23:37:26,643:INFO:Checking exceptions
2024-11-10 23:37:26,643:INFO:Preloading libraries
2024-11-10 23:37:26,644:INFO:Set up data.
2024-11-10 23:37:26,648:INFO:Set up index.
2024-11-10 23:48:36,563:INFO:Initializing predict_model()
2024-11-10 23:48:36,564:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1609ed0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163f910>)
2024-11-10 23:48:36,564:INFO:Checking exceptions
2024-11-10 23:48:36,564:INFO:Preloading libraries
2024-11-10 23:48:36,564:INFO:Set up data.
2024-11-10 23:48:36,567:INFO:Set up index.
2024-11-10 23:48:40,277:INFO:Initializing predict_model()
2024-11-10 23:48:40,278:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160b370>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:48:40,278:INFO:Checking exceptions
2024-11-10 23:48:40,278:INFO:Preloading libraries
2024-11-10 23:48:40,278:INFO:Set up data.
2024-11-10 23:48:40,284:INFO:Set up index.
2024-11-10 23:48:52,180:INFO:Initializing predict_model()
2024-11-10 23:48:52,180:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1609630>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163ff40>)
2024-11-10 23:48:52,180:INFO:Checking exceptions
2024-11-10 23:48:52,180:INFO:Preloading libraries
2024-11-10 23:48:52,180:INFO:Set up data.
2024-11-10 23:48:52,184:INFO:Set up index.
2024-11-10 23:48:52,970:INFO:Initializing predict_model()
2024-11-10 23:48:53,110:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160a6e0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:48:53,110:INFO:Checking exceptions
2024-11-10 23:48:53,110:INFO:Preloading libraries
2024-11-10 23:48:53,110:INFO:Set up data.
2024-11-10 23:48:53,113:INFO:Set up index.
2024-11-10 23:48:53,969:INFO:Initializing predict_model()
2024-11-10 23:48:53,970:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1609c60>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:48:53,970:INFO:Checking exceptions
2024-11-10 23:48:53,970:INFO:Preloading libraries
2024-11-10 23:48:53,972:INFO:Set up data.
2024-11-10 23:48:53,980:INFO:Set up index.
2024-11-10 23:48:54,162:INFO:Initializing predict_model()
2024-11-10 23:48:54,162:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef16081c0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:48:54,162:INFO:Checking exceptions
2024-11-10 23:48:54,162:INFO:Preloading libraries
2024-11-10 23:48:54,162:INFO:Set up data.
2024-11-10 23:48:54,183:INFO:Set up index.
2024-11-10 23:49:00,775:INFO:Initializing predict_model()
2024-11-10 23:49:00,775:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1608d00>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fb50>)
2024-11-10 23:49:00,775:INFO:Checking exceptions
2024-11-10 23:49:00,775:INFO:Preloading libraries
2024-11-10 23:49:00,776:INFO:Set up data.
2024-11-10 23:49:00,778:INFO:Set up index.
2024-11-10 23:49:01,684:INFO:Initializing predict_model()
2024-11-10 23:49:01,684:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160bb80>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:01,684:INFO:Checking exceptions
2024-11-10 23:49:01,685:INFO:Preloading libraries
2024-11-10 23:49:01,686:INFO:Set up data.
2024-11-10 23:49:01,711:INFO:Set up index.
2024-11-10 23:49:04,970:INFO:Initializing predict_model()
2024-11-10 23:49:04,970:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160a8f0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:04,970:INFO:Checking exceptions
2024-11-10 23:49:04,970:INFO:Preloading libraries
2024-11-10 23:49:04,970:INFO:Set up data.
2024-11-10 23:49:04,977:INFO:Set up index.
2024-11-10 23:49:13,746:INFO:Initializing predict_model()
2024-11-10 23:49:13,746:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160a050>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163feb0>)
2024-11-10 23:49:13,746:INFO:Checking exceptions
2024-11-10 23:49:13,746:INFO:Preloading libraries
2024-11-10 23:49:13,746:INFO:Set up data.
2024-11-10 23:49:13,749:INFO:Set up index.
2024-11-10 23:49:14,755:INFO:Initializing predict_model()
2024-11-10 23:49:14,755:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1608bb0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:14,755:INFO:Checking exceptions
2024-11-10 23:49:14,755:INFO:Preloading libraries
2024-11-10 23:49:14,756:INFO:Set up data.
2024-11-10 23:49:14,764:INFO:Set up index.
2024-11-10 23:49:15,186:INFO:Initializing predict_model()
2024-11-10 23:49:15,186:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160b7c0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:15,186:INFO:Checking exceptions
2024-11-10 23:49:15,186:INFO:Preloading libraries
2024-11-10 23:49:15,186:INFO:Set up data.
2024-11-10 23:49:15,189:INFO:Set up index.
2024-11-10 23:49:15,354:INFO:Initializing predict_model()
2024-11-10 23:49:15,354:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1609a50>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:15,354:INFO:Checking exceptions
2024-11-10 23:49:15,354:INFO:Preloading libraries
2024-11-10 23:49:15,354:INFO:Set up data.
2024-11-10 23:49:15,357:INFO:Set up index.
2024-11-10 23:49:15,526:INFO:Initializing predict_model()
2024-11-10 23:49:15,526:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160a410>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:15,526:INFO:Checking exceptions
2024-11-10 23:49:15,526:INFO:Preloading libraries
2024-11-10 23:49:15,527:INFO:Set up data.
2024-11-10 23:49:15,529:INFO:Set up index.
2024-11-10 23:49:20,501:INFO:Initializing predict_model()
2024-11-10 23:49:20,501:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160aaa0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:20,502:INFO:Checking exceptions
2024-11-10 23:49:20,502:INFO:Preloading libraries
2024-11-10 23:49:20,502:INFO:Set up data.
2024-11-10 23:49:20,511:INFO:Set up index.
2024-11-10 23:49:21,066:INFO:Initializing predict_model()
2024-11-10 23:49:21,066:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160a620>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:21,066:INFO:Checking exceptions
2024-11-10 23:49:21,066:INFO:Preloading libraries
2024-11-10 23:49:21,067:INFO:Set up data.
2024-11-10 23:49:21,069:INFO:Set up index.
2024-11-10 23:49:24,746:INFO:Initializing predict_model()
2024-11-10 23:49:24,746:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160b4c0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:24,746:INFO:Checking exceptions
2024-11-10 23:49:24,746:INFO:Preloading libraries
2024-11-10 23:49:24,747:INFO:Set up data.
2024-11-10 23:49:24,753:INFO:Set up index.
2024-11-10 23:49:26,172:INFO:Initializing predict_model()
2024-11-10 23:49:26,172:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1608730>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:26,172:INFO:Checking exceptions
2024-11-10 23:49:26,172:INFO:Preloading libraries
2024-11-10 23:49:26,173:INFO:Set up data.
2024-11-10 23:49:26,181:INFO:Set up index.
2024-11-10 23:49:29,486:INFO:Initializing predict_model()
2024-11-10 23:49:29,486:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef160bb20>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:29,486:INFO:Checking exceptions
2024-11-10 23:49:29,486:INFO:Preloading libraries
2024-11-10 23:49:29,486:INFO:Set up data.
2024-11-10 23:49:29,489:INFO:Set up index.
2024-11-10 23:49:42,357:INFO:Initializing predict_model()
2024-11-10 23:49:42,357:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1608ca0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fa30>)
2024-11-10 23:49:42,357:INFO:Checking exceptions
2024-11-10 23:49:42,357:INFO:Preloading libraries
2024-11-10 23:49:42,357:INFO:Set up data.
2024-11-10 23:49:42,360:INFO:Set up index.
2024-11-10 23:49:42,921:INFO:Initializing predict_model()
2024-11-10 23:49:42,921:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1609ff0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163fb50>)
2024-11-10 23:49:42,921:INFO:Checking exceptions
2024-11-10 23:49:42,921:INFO:Preloading libraries
2024-11-10 23:49:42,922:INFO:Set up data.
2024-11-10 23:49:42,934:INFO:Set up index.
2024-11-10 23:49:47,542:INFO:Initializing predict_model()
2024-11-10 23:49:47,542:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x709ef1609330>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x709ef163e170>)
2024-11-10 23:49:47,542:INFO:Checking exceptions
2024-11-10 23:49:47,542:INFO:Preloading libraries
2024-11-10 23:49:47,543:INFO:Set up data.
2024-11-10 23:49:47,545:INFO:Set up index.
2024-11-10 23:50:25,318:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:50:26,216:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:50:26,216:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:50:26,216:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:50:26,216:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:50:26,669:INFO:Initializing load_model()
2024-11-10 23:50:26,669:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 23:50:44,563:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:50:45,524:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:50:45,524:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:50:45,524:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:50:45,524:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:50:46,016:INFO:Initializing load_model()
2024-11-10 23:50:46,016:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 23:52:09,979:INFO:Initializing predict_model()
2024-11-10 23:52:09,979:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x759c51efca30>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x759c51e01900>)
2024-11-10 23:52:09,979:INFO:Checking exceptions
2024-11-10 23:52:09,979:INFO:Preloading libraries
2024-11-10 23:52:09,979:INFO:Set up data.
2024-11-10 23:52:09,987:INFO:Set up index.
2024-11-10 23:53:54,688:INFO:Initializing predict_model()
2024-11-10 23:53:54,710:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x759c51d1afb0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x759c51d4ff40>)
2024-11-10 23:53:54,710:INFO:Checking exceptions
2024-11-10 23:53:54,710:INFO:Preloading libraries
2024-11-10 23:53:54,711:INFO:Set up data.
2024-11-10 23:53:54,713:INFO:Set up index.
2024-11-10 23:55:13,260:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:55:15,491:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:55:16,669:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:55:17,487:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:17,488:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:17,488:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:17,488:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:18,483:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:55:19,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:19,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:19,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:19,377:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:19,946:INFO:Initializing load_model()
2024-11-10 23:55:19,946:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 23:55:21,479:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:55:23,281:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:55:24,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:24,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:24,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:24,278:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:25,272:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:55:26,562:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-11-10 23:55:27,521:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:27,521:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:27,521:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:27,521:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-11-10 23:55:27,996:INFO:Initializing load_model()
2024-11-10 23:55:27,996:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-11-10 23:55:33,449:INFO:Initializing predict_model()
2024-11-10 23:55:33,449:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7bc7ca5e8250>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x7bc7ca43fac0>)
2024-11-10 23:55:33,450:INFO:Checking exceptions
2024-11-10 23:55:33,450:INFO:Preloading libraries
2024-11-10 23:55:33,450:INFO:Set up data.
2024-11-10 23:55:33,454:INFO:Set up index.
2024-12-02 12:50:29,859:WARNING:/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4
  warnings.warn(f"A NumPy version >={np_minversion} and <{np_maxversion}"

2024-12-02 12:50:30,948:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-02 12:50:30,948:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-02 12:50:30,948:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-02 12:50:30,948:WARNING:
'cuml' is a soft dependency and not included in the pycaret installation. Please run: `pip install cuml` to install.
2024-12-02 12:50:31,544:INFO:Initializing load_model()
2024-12-02 12:50:31,544:INFO:load_model(model_name=bestModel, platform=None, authentication=None, verbose=True)
2024-12-02 12:50:54,950:INFO:Initializing predict_model()
2024-12-02 12:50:54,950:INFO:predict_model(self=<pycaret.classification.oop.ClassificationExperiment object at 0x7390819086a0>, estimator=Pipeline(memory=Memory(location=None),
         steps=[('numerical_imputer',
                 TransformerWrapper(include=['Age', 'Gender', 'Ethnicity',
                                             'ParentalEducation',
                                             'StudyTimeWeekly', 'Absences',
                                             'Tutoring', 'ParentalSupport',
                                             'Extracurricular', 'Sports',
                                             'Music', 'Volunteering', 'GPA'],
                                    transformer=SimpleImputer())),
                ('categorical_imputer',
                 TransformerWrapper(include=[],
                                    transformer=SimpleImputer(strategy='most_frequent'))),
                ('trained_model',
                 GradientBoostingClassifier(random_state=123))]), probability_threshold=None, encoded_labels=False, raw_score=False, round=4, verbose=True, ml_usecase=None, preprocess=True, encode_labels=<function _SupervisedExperiment.predict_model.<locals>.encode_labels at 0x739081a11900>)
2024-12-02 12:50:54,950:INFO:Checking exceptions
2024-12-02 12:50:54,951:INFO:Preloading libraries
2024-12-02 12:50:54,951:INFO:Set up data.
2024-12-02 12:50:54,954:INFO:Set up index.
